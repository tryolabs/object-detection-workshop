{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Faster R-CNN\n",
    "\n",
    "The objective of this activity is to implement the main parts of the Faster R-CNN algorithm.\n",
    "\n",
    "We will:\n",
    "* understand intuitively how the parts of the algorithm are working and how they fit together.\n",
    "* implement all the stages required for **inference** one by one, using Python, leveraging numpy and TensorFlow.\n",
    "* use **existing weights** of a Faster R-CNN model that has been trained on the [COCO Dataset](http://cocodataset.org/), to guide and facilitate the process.\n",
    "\n",
    "We will **NOT**:\n",
    "* implement any training code whatsoever. We won't code any loss function or deal with ground truth boxes.\n",
    "* train the model, as we already have weights for you that work.\n",
    "\n",
    "We've tried to keep code in the notebooks to a minimum, mainly data manipulation and visualization, to make it easy enough to follow. All accompanying code is under the `workshop` Python package.\n",
    "\n",
    "After some introductory code, the notebook will continue as follows:\n",
    "* Playing with a **pre-trained ResNet** to obtain features out of an image.\n",
    "* Generate regions of interest by implementing the **Region Proposal Network** detailed in [1].\n",
    "* Prepare this regions to be fed to the second stage, by applying **RoI pooling**.\n",
    "* Classify and refine said regions by passing them through an **R-CNN**, as detailed in [2].\n",
    "\n",
    "We'll present you with stubs for the different functions required and your task will be to fill them in.\n",
    "\n",
    "* [1] Ren, Shaoqing, et al. *Faster R-CNN: Towards real-time object detection with region proposal networks.*\n",
    "* [2] Girshick, Ross. *Fast R-CNN.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The basics\n",
    "We'll start with some imports.\n",
    "\n",
    "The local imports are under the `workshop` package, which you should have installed using `pip install -e workshop/` in the environment you're running your notebook on.\n",
    "\n",
    "Within `workshop` we have some modules:\n",
    "* `vis`: various visualization utilities to draw bounding boxes, sliders, etc.\n",
    "* `image`: utilities for reading images and loading them into PIL (the imaging library).\n",
    "* `resnet`: the implementation for the base network we're going to use (more on this shortly).\n",
    "* `faster`: utilities and parts we won't be implementing but provide for completeness' sake.\n",
    "\n",
    "Let's test some things to make sure everything is up and ready to go.\n",
    "\n",
    "Start by running the following in your terminal, and then test the rest of the imports:\n",
    "```bash\n",
    " $ jupyter nbextension enable --py widgetsnbextension\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Checkbox, FloatSlider, Layout\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Try to enable TF eager execution, or do nothing if running again.\n",
    "try:\n",
    "    tf.enable_eager_execution()\n",
    "except ValueError:\n",
    "    # Already executed.\n",
    "    pass\n",
    "\n",
    "\n",
    "# Local imports.\n",
    "from workshop.faster import (\n",
    "    clip_boxes, rcnn_proposals, run_base_network, run_resnet_tail,\n",
    "    generate_anchors_reference, sort_anchors\n",
    ")\n",
    "from workshop.image import open_all_images, open_image, to_image\n",
    "from workshop.vis import (\n",
    "    add_rectangle, draw_bboxes, draw_bboxes_with_labels, image_grid,\n",
    "    pager, vis_anchors\n",
    ")\n",
    "\n",
    "# Notebook-specific settings.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now load some images to play with and display them below. Change which image is passed to the `to_image` function to see it in full size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = open_all_images('images/')\n",
    "\n",
    "axes = image_grid(len(images))\n",
    "for ax, (name, image) in zip(axes, images.items()):\n",
    "    ax.imshow(np.squeeze(image))\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.subplots_adjust(wspace=.01)\n",
    "plt.show()\n",
    "\n",
    "image = images['woman']\n",
    "\n",
    "# `to_image` turns a `numpy.ndarray` into a PIL image, so it's displayed by the notebook.\n",
    "to_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# The base network: ResNet\n",
    "\n",
    "\n",
    "The basis for the Faster R-CNN algorithm is to leverage a pre-trained classifier network to extract feature maps (also called *activation maps*) from the image. For this implementation, we'll be using the popular ResNet 101 [3].\n",
    "\n",
    "We provide the implementation itself (which you can see in the `workshop.resnet` module), as well as a checkpoint with the pre-trained weights (in the `checkpoint/` directory).\n",
    "\n",
    "---\n",
    "\n",
    "### Aside\n",
    "\n",
    "The ResNet architecture consists of four stacked **blocks**, after which a fully-connected layer is attached. As is expected of CNNs, these blocks detect features from most simple to most complex. For this part of the algorithm, we're using the output of the **block 3**, so we get somewhat generic features. The intuition is that, if we go all the way and use block 4, we might have things that are too specific to the dataset used to pre-train the ResNet (the Imagenet dataset) and thus not as desirable for a network that wants to identify generic objects. \n",
    "\n",
    "---\n",
    "\n",
    "Run the base network on different images, in order to see how the different activation maps behave. **Can you notice any particular features being detected in the activation maps?**\n",
    "\n",
    "* [3] He, Kaiming, et al. *Deep residual learning for image recognition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    feature_map = run_base_network(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact(page=pager(1024, 20, 'Feature map'))\n",
    "def display_feature_maps(page):\n",
    "    axes = image_grid(20)\n",
    "    for idx, ax in enumerate(axes):\n",
    "        if page * 20 + idx >= 1024:\n",
    "            break\n",
    "        ax.imshow(\n",
    "            feature_map.numpy()[0, :, :, page * 20 + idx],\n",
    "            cmap='gray', aspect='auto'\n",
    "        )\n",
    "\n",
    "    plt.subplots_adjust(wspace=.01, hspace=.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn: understand what patterns activate particular filters.\n",
    "\n",
    "Let's overlay the feature maps into the images themselves, so we can take a more detailed look into what pattern makes the ResNet react.\n",
    "\n",
    "See, for example:\n",
    "* Feature map 171 in `woman`.\n",
    "* Feature maps 19, 22 in `cats`.\n",
    "* Feature map 34, 64 in `bicycles`.\n",
    "* Feature map 253 in `kids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(idx=pager(1024, 1, 'Feature map index'))\n",
    "def overlay_feature_map(idx):\n",
    "    # Normalize the feature map so we get the whole range of colors.\n",
    "    fm = (\n",
    "        feature_map.numpy()[0, :, :, idx]\n",
    "        / feature_map.numpy()[0, :, :, idx].max()\n",
    "        * 255\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    # Resize the feature map without interpolation.\n",
    "    fm_image = Image.fromarray(fm, mode='L').convert('RGBA')\n",
    "    fm_image = fm_image.resize(image.shape[1:3][::-1], resample=Image.NEAREST)\n",
    "    \n",
    "    # Add some alpha to overlay it over the image.\n",
    "    fm_image.putalpha(200)\n",
    "    \n",
    "    base_image = to_image(image)\n",
    "    base_image.paste(fm_image, (0, 0), fm_image)\n",
    "    \n",
    "    return base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section didn't require any implementation at all, but get ready, because we're about to. The main idea here was illustrating what we mean when we say that the later layers of a classification network are **feature detectors**, reacting to particular patterns in an image.\n",
    "\n",
    "What would you do if you were to use this information to detect objects? How could you leverage the fact that we can say \"there's a cat ear here!\"? We're going to explore these questions in the following sections.\n",
    "\n",
    "For now, back to the slides!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Finding stuff with the Region Proposal Network\n",
    "\n",
    "Having gone through the theory, we'll now turn our attention to implementing a **Region Proposal Network**. The idea, as we've seen, is to use the feature maps provided by the ResNet to find out **where** there might be an object located.\n",
    "\n",
    "This is where **anchors** come into play. We'll take a grid of points over the image and consider several anchors (also called \"reference boxes\" sometimes) for each of them (15 in this case). The RPN layers themselves will then predict whether there's an object in each of these 15 boxes **and** how much we need to resize them to better fit it.\n",
    "\n",
    "The tasks we have ahead of us are, thus:\n",
    "* Get the **coordinates** $(x_{min}, y_{min}, x_{max}, y_{max})$ for each of the anchors. There are $15$ anchors and the centers will be separated by approximately $16$ pixels, so we're talking about several thousand of coordinates.\n",
    "* Find out how to do the special **encoding** and **decoding** of coordinates described in the Faster R-CNN paper, so the RPN can predict locations in the image.\n",
    "* Build the **convolutional layers** comprising the RPN and run them through different images.\n",
    "* **Translate the predictions** of the RPN layer into usable proposals.\n",
    "\n",
    "### Note: coordinate conventions\n",
    "Except in specific cases, we'll be using the convention $(x_{min}, y_{min}, x_{max}, y_{max})$ to denote a bounding box, were $(x_{min}, y_{min})$ corresponds to the top-left point and $(x_{max}, y_{max})$ the bottom right.\n",
    "\n",
    "As usual with image processing, the origin of the coordinate system, $(0, 0)$, is on the top-left of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating anchors\n",
    "\n",
    "We'll get the anchor's coordinates in two steps. First, we will use a function called `generate_anchors_reference` which, given the anchors' settings (i.e. size, aspect ratio, scales), returns an array with the coordinates for the boxes (in pixel space) assuming they're centered around (0, 0). This will give us, effectively, a $(15, 4)$ array.\n",
    "\n",
    "There are three settings for the anchors:\n",
    "\n",
    "* `base_size`: **side length for a square anchor**, in pixels (e.g. 256). Increasing it makes the anchor cover more area of the image.\n",
    "* `scales`: **scale factors** to consider taking `base_size` as reference. For instance, a scale of `2` will make the effective size `512` if base size was `256`.\n",
    "* `aspect_ratios`: **aspect ratios** of the anchors, expressed as the value of `height / width`. Note that *changing the aspect ratio doesn't change the area the anchor covers*. An aspect ratio of `2` means that, for the area covered by a square anchor of of `base_size`, we should get a rectangle of twice the height than width.\n",
    "\n",
    "Let's see how this looks like using a single aspect ratio and 3 scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_ref = generate_anchors_reference(\n",
    "    256,  # Base size.\n",
    "    [1],  # Aspect ratios.\n",
    "    [0.5, 1, 2],  # Scales.\n",
    ")\n",
    "\n",
    "vis_anchors(anchors_ref)\n",
    "\n",
    "# Remember this is just a numpy array of shape\n",
    "#     (total_aspect_ratios * total_scales, 4)\n",
    "# with the corner points of the reference anchors using the\n",
    "# convention (x_min, y_min, x_max, y_max).\n",
    "print('As a numpy array:')\n",
    "print(anchors_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now change the aspect ratio, but keep the same scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_ref = generate_anchors_reference(\n",
    "    256,  # Base size.\n",
    "    [0.5],  # Aspect ratios.\n",
    "    [0.5, 1, 2],  # Scales.\n",
    ")\n",
    "\n",
    "vis_anchors(anchors_ref)\n",
    "\n",
    "print('As a numpy array:')\n",
    "print(anchors_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are now using $0.5$ as aspect ratio, it means the width/height relation for each anchor should be equal to that (so the rectangles are elongated).\n",
    "\n",
    "Now, let's try using a single scale, but varying the aspect ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_ref = generate_anchors_reference(\n",
    "    256,  # Base size.\n",
    "    [0.5, 1, 2],  # Aspect ratios.\n",
    "    [2],  # Scales.\n",
    ")\n",
    "\n",
    "vis_anchors(anchors_ref)\n",
    "\n",
    "print('As a numpy array:')\n",
    "print(anchors_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, every anchor depicted here should have the same area, since they are all the same **scale** and only vary in their **aspect ratio**.\n",
    "\n",
    "Finally, let's generate the final set of **15 anchor references** centered around (0,0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_ref = generate_anchors_reference(\n",
    "    256,  # Base size.\n",
    "    [0.5, 1, 2],  # Aspect ratios.\n",
    "    [0.125, 0.25, 0.5, 1, 2],  # Scales.\n",
    ")\n",
    "\n",
    "vis_anchors(anchors_ref)\n",
    "\n",
    "print('As a numpy array:')\n",
    "print(anchors_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some perspective, let's draw these anchors over **a single arbitrary point** in the image, to see how they match.\n",
    "\n",
    "\n",
    "Keep in mind that `anchors_ref` is a numpy array containing 15 values, where each one is a rectangle represented as $(x_{min}, y_{min}, x_{max}, y_{max})$.\n",
    "\n",
    "\n",
    "Since the anchor references are **centered around $(0, 0)$**, it is easy to translate them over any point $P$ by just adding up the coordinates: the anchor $(x_{min}, y_{min}, x_{max}, y_{max})$ at point $P = (x_p, y_p)$ would be specified by the coordinates $(x_{min} + x_p, y_{min} + y_p, x_{max} + x_p, y_{max} + y_p)$.\n",
    "\n",
    "Let's see how this looks like at point $(400, 270)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = np.array([400, 270])\n",
    "\n",
    "# Sum the point on both the *_min and the *_max parts.\n",
    "anchors_at_point = anchors_ref + np.concatenate([point, point])\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 20))\n",
    "ax.imshow(to_image(image))\n",
    "\n",
    "# Add a buffer around the image so we see the whole anchor references.\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "for idx in range(anchors_at_point.shape[0]):\n",
    "    add_rectangle(ax, anchors_at_point[idx, :])\n",
    "\n",
    "# Plot the reference point in use.\n",
    "ax.plot(point[0], point[1], marker='s', color='#dc3912', markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the larger boxes cover quite a bit of the image, while the smaller ones will be useful for detecting very small objects.\n",
    "\n",
    "\n",
    "Now, our **first real coding task** (yes!) will be to do the same with the anchor references over each of the **anchor centers** in the image.\n",
    "\n",
    "\n",
    "Given that we're using a ResNet 101, which has a downsampling factor of 16 (i.e. every point in the feature map --block 3 as we said-- corresponds to a $16\\times16$ region of the original image), we'll select the centers **every 16 pixels** in each direction.\n",
    "\n",
    "For reference, the anchor centers are visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is actually defined within `run_base_network`, but for visualization\n",
    "# purposes, we're defining it again.\n",
    "OUTPUT_STRIDE = 16\n",
    "\n",
    "# Print the anchor centers in use.\n",
    "_, ax = plt.subplots(1, figsize=(16, 20))\n",
    "\n",
    "ax.imshow(to_image(image))\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "for x in range(0, image.shape[2], OUTPUT_STRIDE):\n",
    "    for y in range(0, image.shape[1], OUTPUT_STRIDE):\n",
    "        ax.plot(x, y, marker='s', color='#dc3912', markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this part by getting the entire list of anchors for the image. This will be done within the `generate_anchors` function.\n",
    "\n",
    "### Programming task: implement `generate_anchors` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the anchor properties that will be used in our implementation.\n",
    "# Compared to the values picked in the original Faster R-CNN paper, we've\n",
    "# added two smaller scales that help the model detect the smaller objects\n",
    "# present in the COCO dataset.\n",
    "ANCHOR_BASE_SIZE = 256\n",
    "ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "ANCHOR_SCALES = [0.125, 0.25, 0.5, 1, 2]\n",
    "\n",
    "\n",
    "def generate_anchors(feature_map_shape):\n",
    "    \"\"\"Generate anchors for an image.\n",
    "\n",
    "    Using the feature map (the output of the pretrained network for an image)\n",
    "    and the anchor references (generated using the specified anchor sizes and\n",
    "    ratios), we generate a list of anchors.\n",
    "\n",
    "    Anchors are just fixed bounding boxes of different ratios and sizes that\n",
    "    are uniformly generated throughout the image.\n",
    "\n",
    "    Arguments:\n",
    "        feature_map_shape: Shape of the convolutional feature map used as\n",
    "            input for the RPN.\n",
    "            Should be (batch, feature_height, feature_width, depth).\n",
    "\n",
    "    Returns:\n",
    "        all_anchors: A Tensor with the anchors at every spatial position, of\n",
    "            shape `(feature_height, feature_width, num_anchors_per_points, 4)`\n",
    "            using the (x1, y1, x2, y2) convention.\n",
    "    \"\"\"\n",
    "\n",
    "    anchor_reference = generate_anchors_reference(\n",
    "        ANCHOR_BASE_SIZE, ANCHOR_RATIOS, ANCHOR_SCALES\n",
    "    )\n",
    "    \n",
    "    # Tip: first, implement it with regular Python loops.\n",
    "    #\n",
    "    # If you have time and want to try to do it in a vectorized way, the\n",
    "    # following functions might be of use: `tf.meshgrid`, `tf.range`,\n",
    "    # `tf.expand_dims` and `tf.transpose`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "\n",
    "anchors = tf.reshape(generate_anchors(feature_map.shape), [-1, 4])\n",
    "\n",
    "print('Anchors (real image size):')\n",
    "print()\n",
    "print(anchors.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the anchors over an arbitrary point to corroborate that the results makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the anchors. Try changing to different points of the image.\n",
    "# Note that we're referring to _positions in the feature map_ here, so the\n",
    "# actual point in the image will be around `OUTPUT_STRIDE` times the value.\n",
    "point = np.array([30, 20])\n",
    "\n",
    "# Reshape back to (H, W, num_anchors, 4) so we can easily get a given point's anchors.\n",
    "anchors_at_point = anchors.numpy().reshape(\n",
    "    (feature_map.shape[1], feature_map.shape[2], 15, 4)\n",
    ")[point[1], point[0], :, :]\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 20))\n",
    "\n",
    "ax.imshow(to_image(image))\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "for idx in range(anchors_at_point.shape[0]):\n",
    "    add_rectangle(ax, anchors_at_point[idx, :])\n",
    "\n",
    "# Plot the reference point in use.\n",
    "ax.plot(\n",
    "    point[0] * OUTPUT_STRIDE,\n",
    "    point[1] * OUTPUT_STRIDE,\n",
    "    marker='s', color='#dc3912', markersize=3\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we've finished generating the anchors that will be used by the RPN. This is, effectively, a list of $15 \\times F_x \\times F_y$, where $F_x, F_y$ are the feature map width and height, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and decoding bounding box coordinates\n",
    "\n",
    "\n",
    "Deep neural networks usually train and converge better when their outputs have zero mean and unit variance (and/or their intermediate values do so). Due to this, and the difficulty in predicting values in a possibly unbounded region (pixel coordinates), a special encoding is applied to the coordinates before passing them in to the network (and after getting them out).\n",
    "\n",
    "The idea behind the encoding is to express the coordinates of a bounding box $B$ as a set of four numbers $(D_x, D_y, D_w, D_h)$ (the **deltas**) and a reference anchor $R$. $D_x$ and $D_y$ indicate how much the center of $R$ should be moved to reach the center of $B$, normalized by the size of $R$, while $D_w$ and $D_h$ indicate how much the width and height of $R$ must be increased or decreased to reach the size of $B$ (it's actually the log of that value, as you'll see below).\n",
    "\n",
    "For the following equations, we change from the $(x_{min}, y_{min}, x_{max}, y_{max})$ encoding to the **center+dimensions encoding** $(x, y, w, h)$, where $(x, y)$ are the **center coordinates**, and $(w, h)$ the **width and height**. The equations to encode $B = (x_b, y_b, w_b, h_b)$ with respect to anchor $R = (x_r, y_r, w_r, h_r)$ are, then:\n",
    "\n",
    "$D_x = \\frac{x_b - x_r}{w_r} \\quad$\n",
    "$D_y = \\frac{y_b - y_r}{h_r} \\quad$\n",
    "$D_w = log \\frac{w_b}{w_r} \\quad$\n",
    "$D_h = log \\frac{h_b}{h_r}$\n",
    "\n",
    "The equations to decode $B = (x_b, y_b, w_b, h_b)$ given $R = (x_r, y_r, w_r, h_r)$ and deltas $D = (D_x, D_y, D_w, D_h)$ are:\n",
    "\n",
    "$x_b = D_x w_r + x_r \\quad$\n",
    "$y_b = D_y h_r + y_r \\quad$\n",
    "$w_b = e^{D_w} w_r \\quad$\n",
    "$h_b = e^{D_h} h_r \\quad$\n",
    "\n",
    "We'll implement two functions here, `encode` and `decode`. While only the latter will be used, it's useful to implement both in order to understand the whole process and to make it easier to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task: implement `get_dimensions_and_center` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You might find it useful to implement the following function first in order\n",
    "# to obtain the dimensions (width and height) and center of a bounding box,\n",
    "# required for calculating the deltas in `encode` and `decode`.\n",
    "def get_dimensions_and_center(bboxes):\n",
    "    \"\"\"Obtain width, height and center coordinates of a bounding box.\n",
    "    \n",
    "    Arugments:\n",
    "        bboxes: Tensor of shape (num_bboxes, 4).\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of Tensors of shape (num_bboxes,) with the values\n",
    "        width, height, center_x and center_y corresponding to each\n",
    "        bounding box.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: Fully read the docstring above.\n",
    "    # Tip: You may find the Tensorflow function `tf.split` useful.\n",
    "\n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return width, height, ctx, cty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task: implement `encode` function and play around with the checks at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(anchors, bboxes):\n",
    "    \"\"\"Encode bounding boxes as deltas w.r.t. anchors.\n",
    "    \n",
    "    Arguments:\n",
    "        anchors: Tensor of shape (num_bboxes, 4). With the same bbox\n",
    "            encoding.\n",
    "        bboxes: Tensor of shape (num_bboxes, 4). Having the bbox\n",
    "            encoding in the (x_min, y_min, x_max, y_max) order.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (num_bboxes, 4) with the different deltas needed\n",
    "            to transform `anchors` to `bboxes`. These deltas are with\n",
    "            regard to the center, width and height of the two boxes.\n",
    "    \"\"\"\n",
    "   \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "# Encoding `bbox` with respect to an anchor having the same center\n",
    "# should keep the first two deltas at zero.\n",
    "anchor = np.array([[0, 0, 100, 100]], dtype=np.float32)\n",
    "bbox = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "print('With same center, first two deltas should be zero:\\n', encode(anchor, bbox).numpy())\n",
    "print()\n",
    "\n",
    "# Encoding `bbox` with respect to an anchor having the same size\n",
    "# should keep the last two deltas at zero.\n",
    "anchor = np.array([[0, 0, 100, 100]], dtype=np.float32)\n",
    "bbox = np.array([[50, 50, 150, 150]], dtype=np.float32)\n",
    "print('Same size, last two deltas should be zero:\\n', encode(anchor, bbox).numpy())\n",
    "\n",
    "# What other ways to check the functions can you think of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task: implement `decode` function, play around with the checks at the bottom.\n",
    "### Then, verify that that the round trip `encode -> decode` works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(anchors, deltas):\n",
    "    \"\"\"Decode bounding boxes by applying deltas to anchors.\n",
    "    \n",
    "    Arguments:\n",
    "        anchors: Tensor of shape (num_bboxes, 4). Having the bbox\n",
    "            encoding in the (x_min, y_min, x_max, y_max) order.\n",
    "        deltas: Tensor of shape (num_bboxes, 4). Deltas (as returned by\n",
    "            `encode`) that we want to apply to `bboxes`.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (num_bboxes, 4) with the decoded proposals,\n",
    "            obtained by applying `deltas` to `anchors`.\n",
    "    \"\"\"\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# Decoding `anchor` with zero `deltas` should keep the box as-is.\n",
    "anchor = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "delta = np.array([[0, 0, 0, 0]], dtype=np.float32)\n",
    "print('Zero delta, should get a bounding box with same dimensions:')\n",
    "print('\\tAnchor:', anchor)\n",
    "print('\\tBounding box:', decode(anchor, delta).numpy())\n",
    "print()\n",
    "\n",
    "# Applying a `delta` with two ones at first then two zeros to an `anchor`\n",
    "# should get a bounding box of the size but moved to the right one-width.\n",
    "anchor = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "delta = np.array([[1, 1, 0, 0]], dtype=np.float32)\n",
    "print('First-two are ones, obtained box moved to the right one width:')\n",
    "print('\\tAnchor:', anchor)\n",
    "print('\\tBounding box:', decode(anchor, delta).numpy())\n",
    "print()\n",
    "\n",
    "# Decoding `anchor` with two zeros at first then two ones at `deltas`\n",
    "# should get a larger bounding box while maintaining the center.\n",
    "anchor = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "delta = np.array([[0, 0, 1, 1]], dtype=np.float32)\n",
    "print('Last-two are ones, center should be the same:')\n",
    "print('\\tAnchor:', anchor)\n",
    "print('\\tBounding box:', decode(anchor, delta).numpy())\n",
    "\n",
    "# What other ways to check the functions can you think of? How can\n",
    "# you pick the deltas so that it exactly doubles in size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the round-trip of `encode` and `decode`, to see if they're consistent between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the round-trip: encode `bboxes` w.r.t. the anchors `anchors`,\n",
    "# which gives us the deltas that transform `anchors` into `bboxes`.\n",
    "# Then decode the `anchors` with said deltas to see that, effectively,\n",
    "# we get `bboxes` back.\n",
    "anchor = np.array([\n",
    "    [0, 0, 100, 100],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# You can try out other bounding boxes, just make sure to respect the\n",
    "# convention of first putting (x_min, y_min) then (x_max, y_max), or\n",
    "# you may get an invalid bounding box.\n",
    "bboxes = np.array([\n",
    "    [25, 25, 75, 75],\n",
    "    [10, -205, 120, 20],\n",
    "    [-35, 37, 38, 100],\n",
    "    [-0.2, -0.2, 0.2, 0.2],\n",
    "    [-25, -50, -5, -20],\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(\n",
    "    'Round-trip looks good:',\n",
    "    np.sum(np.abs(\n",
    "        decode(anchor, encode(anchor, bboxes)) - bboxes\n",
    "    )) < 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time left at the end, you could try to gain further intuition on what they do and what the encoding's edge cases and limitations are by looking at more examples and plotting the deltas as `bboxes` moves through the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layers\n",
    "\n",
    "We now have a variable-size feature map (a factor of 16 times spatially smaller than the original image) and we want to predict, for each spatial position, how to modify (i.e. the $4$ values from above, $D_{x, y, w, h}$) each of the $k = 15$ anchors. In this context, it makes sense to use a convolutional layer (or more) on the feature map, where the final number of filters will be $4 \\times k$.\n",
    "\n",
    "For each of these anchors we'll also want to decide whether we think there's an object present on said region or not (thus, $2 \\times k$ more filters). This will, in essence, look at the activation maps we saw before and decide whether, in a given region, the activated features amount to an object being in there (e.g. many _cat face_ features have been activated, so there's probably an object in that region).\n",
    "\n",
    "As we saw in the slides, the RPN first has a $3\\times3$ convolutional layer with $512$ filters and then two outputs heads:\n",
    "* One with $2 \\times k$ filters for the **objectness score**.\n",
    "* One with $4 \\times k$ filters for the **encoded deltas**.\n",
    "\n",
    "Both will be implemented as $1 \\times 1$ convolutions in order to support variable-size images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task: implement `run_rpn` function (the forward pass of a RPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that when implementing Faster R-CNN for training, we should\n",
    "# also specify initializers and regularizers for the weights. We're\n",
    "# omitting them here for brevity.\n",
    "\n",
    "def run_rpn(feature_map):\n",
    "    \"\"\"Run the RPN layers through the feature map.\n",
    "    \n",
    "    Will run the input through an initial convolutional layer of\n",
    "    filter size 3x3 and 512 channels, using athe ReLU6 activation.\n",
    "    The output of this layer has the same spatial size as the\n",
    "    input.\n",
    "    \n",
    "    Then run two 1x1 convolutions over this intermediate layer, one\n",
    "    for the resizings and one for the objectness probabilities.\n",
    "    Remember to apply the softmax function over the objectness\n",
    "    scores to get a probability distribution.\n",
    "    \n",
    "    Arguments:\n",
    "        feature_map: Tensor of shape (1, W, H, C), with WxH the\n",
    "            spatial shape of the feature map and C the number of\n",
    "            channels (1024 in this case).\n",
    "            \n",
    "    Returns:\n",
    "        Tuple of Tensors, with the first being the output of the bbox\n",
    "        resizings `(W * H * num_anchors, 4)` while the second being\n",
    "        the objectness score, of size `(W * H * num_anchors, 2)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: Read the docstring thoroughly to help you pass the correct\n",
    "    # parameters to the conv layers, especially padding (you want to\n",
    "    # keep the *same* spatial size after the initial conv layer).\n",
    "    \n",
    "    # Tip: See the functions `tf.layers.conv2d` and `tf.reshape`. Also\n",
    "    # see `tf.nn.softmax` for the softmax function.\n",
    "    \n",
    "    # The names of the layers should be: `rpn/conv` for the base layer,\n",
    "    # `rpn/cls_conv` for the objectness score, and `rpn/bbox_conv` for\n",
    "    # the bbox resizing.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return rpn_bbox_pred, rpn_cls_prob\n",
    "\n",
    "\n",
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    rpn_bbox_pred, rpn_cls_prob = run_rpn(feature_map)\n",
    "    \n",
    "\n",
    "expected_preds = (\n",
    "    feature_map.shape[1]\n",
    "    * feature_map.shape[2]\n",
    "    * len(ANCHOR_RATIOS)\n",
    "    * len(ANCHOR_SCALES)\n",
    ")\n",
    "\n",
    "assert rpn_bbox_pred.shape[0] == expected_preds, 'Number of proposals should match'\n",
    "assert rpn_cls_prob.shape[0] == expected_preds, 'Number of proposals should match'\n",
    "\n",
    "assert rpn_bbox_pred.shape[1] == 4, 'There should be one delta per bbox coordinate (i.e., four)'\n",
    "assert rpn_cls_prob.shape[1] == 2, 'The objectness score should have two outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating proposals from the RPN output\n",
    "\n",
    "We now have the RPN layers outputs as-is. These will be the basis for *regions of interest* that will go through to the next stage of the object detection pipeline.\n",
    "\n",
    "Remember that the RPN layers outputs are the **encoded deltas**. So we need to get them back to image pixel space. When we do this, we can visualize what we have so far!\n",
    "\n",
    "First we decode the outputs of the RPN using our previously-implemented `decode` function, obtaining **proposals**. We also get a single-dimension **objectness score** for each of these proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate proposals from the RPN's output by decoding the bounding boxes\n",
    "# according to the configured anchors.\n",
    "proposals = decode(anchors, rpn_bbox_pred)\n",
    "\n",
    "# Get the (positive-object) scores from the RPN.\n",
    "scores = tf.reshape(rpn_cls_prob[:, 1], [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that we will have **more than 22k proposals** as output, and most will actually be garbage.\n",
    "\n",
    "In order to visualize what we have so far, we need to first sort them by score, and only keep those with the highest score.\n",
    "\n",
    "### Programming task: implement `keep_top_n` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_top_n(proposals, scores, topn):\n",
    "    \"\"\"Keeps only the top `topn` proposals, ordered by score.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals: Tensor of shape (num_proposals, 4), holding the\n",
    "            coordinates of the proposals' bounding boxes.\n",
    "        scores: Tensor of shape (num_proposals,), holding the\n",
    "            scores associated to each bounding box.\n",
    "        topn (int): Number of proposals to keep.\n",
    "        \n",
    "    Returns:\n",
    "        (`min(num_proposals, topn)`, `scores`) ordered by score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tip: See `tf.minimum`, `tf.nn.top_k` to get the top values, and\n",
    "    # `tf.gather` to select indices out of a Tensor.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    return sorted_top_proposals, sorted_top_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn: play around with displaying a different number of proposals\n",
    "\n",
    "Then, answer the following questions:\n",
    "\n",
    "1. What do you see?\n",
    "2. Why does it happen? Does it make sense?\n",
    "3. What problem or problems do we have?\n",
    "4. What would happen if we had initialized the network with random weights instead of using a pre-trained checkpoint?\n",
    "5. How could we fix these issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 3000\n",
    "\n",
    "top_raw_proposals, top_raw_scores = keep_top_n(proposals, scores, topn)\n",
    "# Display the first `topn` proposals, as ordered by score.\n",
    "@interact(\n",
    "    topn=pager(500, 1, min=1, value=10, description='Number of proposals')\n",
    ")\n",
    "def draw(topn):\n",
    "    print('Minimum score: {:.2f}'.format(top_raw_scores[topn]))\n",
    "    return draw_bboxes(image, top_raw_proposals[:topn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn: making sense of the RPN deltas\n",
    "\n",
    "Let's plot a histogram of the bounding box modifications (the deltas) for our current image.\n",
    "\n",
    "Look at the results. Do they make sense? Does it seem that the encoding is indeed helping unbias the predictions? What do values near zero mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rpn_bbox_pred.numpy()\n",
    "\n",
    "_, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    title = ['D_x', 'D_y', 'D_w', 'D_h'][idx]\n",
    "    ax.set_title(title)\n",
    "    ax.hist(preds[:, idx], bins=50)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the objectness scores. As you'll see, most of the anchors are deemed not worthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rpn_cls_prob.numpy()[:, 1]\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 6))\n",
    "ax.set_title('Scores (0 = no object, 1 = object)')\n",
    "ax.hist(preds, bins=100)\n",
    "\n",
    "print('{} predictions over 0.9, out of a total of {}'.format(\n",
    "    len(np.flatnonzero(preds > 0.9)), len(preds)\n",
    "))\n",
    "print()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some time left, it may prove insightful to analyze other statistics, such as the objectness and/or resizing by anchor size, or by position in the image. Performing an analysis like this can help pick hyperparameters, guide improvements for the algorithms and find pathologies on the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering proposals\n",
    "\n",
    "As we saw above, it would be smart to implement a stage where we **filter** the proposals that we have, in order to perform better object detection in a later phase.\n",
    "\n",
    "* Some of the proposals may end up being invalid, as no constraints have been placed on the resizings (aside from the regularization induced by the encoding). For instance, we may end up with **zero-area proposals**, or with the extremes flipped. This may be especially true when we're training the algorithm from scratch (with randomly initialized weights), but we're going to filter them just in case.\n",
    "* Many of the proposals may end up being **very similar to each other**. Due to this, we're going to apply an operation called **non-maximum suppression** to keep only those proposals that are most different to each other, and enable lower-score but more diverse proposals to get into our final set (to improve the quality of our detections).\n",
    "\n",
    "\n",
    "First, we will plot the area per proposal in order to visualize how it is distributed, and see if we have some proposals with zero or negative area (in this case, negative area means that the bounding box extremes were flipped). As we said before, it is very much possible that since we're using fully-trained weights, no proposals with negative area are present. You might want to see the `encode` and `decode` functions you implemented above to see exactly when it can go negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = proposals.numpy()\n",
    "areas = (props[:, 2] - props[:, 0]) * (props[:, 3] - props[:, 1])\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 6))\n",
    "ax.set_title('Area per proposal')\n",
    "ax.hist(areas, bins=100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Proposals with areas under zero:')\n",
    "print(np.flatnonzero(areas <= 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Programming task: implement `filter_proposals` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N.B.: You might as well skip this step if you're running out of time and\n",
    "# there are no proposals with area under zero, but beware that on a real\n",
    "# implementation ignoring this will cause trouble.\n",
    "\n",
    "def filter_proposals(proposals, scores):\n",
    "    \"\"\"Filters non-positive area proposals.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals: Tensor of shape (num_proposals, 4), holding the\n",
    "            coordinates of the proposals' bounding boxes.\n",
    "        scores: Tensor of shape (num_proposals,), holding the\n",
    "            scores associated to each bounding box.\n",
    "        \n",
    "    Returns:\n",
    "        (`proposals`, `scores`), but with non-positive area proposals removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: see `tf.greater`, `tf.maximum`, `tf.boolean_mask`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return proposals, scores\n",
    "\n",
    "\n",
    "# Filter proposals with negative areas.\n",
    "proposals, scores = filter_proposals(proposals, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing redundancy: non-maximum supression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use non-maximum suppression on the list of proposals we have. The end result will be a reduced list of proposals (in fact, of size `POST_NMS_TOP_N` defined below), ordered by objectness score, with some redundancy removed (that is, proposals that are too similar to each other will be discarded).\n",
    "\n",
    "As explained in [1], NMS greedily selects a subset of bounding boxes in descending order of score, pruning away boxes that have high intersection-over-union (IOU) [2] overlap with previously selected boxes.\n",
    "\n",
    "We'll be using `NMS_THRESHOLD` as the **IOU overlap threshold**. Also, in order to speed up the NMS (as we may have tens of thousands of proposals, depending on the image size), we'll first limit our proposal list to the top `PRE_NMS_TOP_N` proposals ordered by score.\n",
    "\n",
    "We'll use an already-implemented Tensorflow function for NMS itself. While this avoids the need to code the algorithm, we need to prepare the parameters correctly to feed it.\n",
    "\n",
    "You can read more about non-maximum suppression [here](https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/) and [here](https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/).\n",
    "\n",
    "* [1] https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression\n",
    "* [2] https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit of the initial proposal list, to reduce the number of proposals fed to\n",
    "# non-maximum suppression.\n",
    "PRE_NMS_TOP_N = 12000\n",
    "\n",
    "# We will use the `keep_top_n` function that you have implemented before!\n",
    "proposals, scores = keep_top_n(proposals, scores, PRE_NMS_TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the proposals pre-filtered, let's now apply NMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task: implement `apply_nms` function, together with the `change_order` helper function which will be useful for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final maximum number of proposals, as returned by NMS.\n",
    "POST_NMS_TOP_N = 2000\n",
    "\n",
    "# IOU overlap threshold for the NMS procedure.\n",
    "NMS_THRESHOLD = 0.7\n",
    "\n",
    "\n",
    "# You might find the following function useful for re-ordening the coordinates\n",
    "# as expected by Tensorflow.\n",
    "def change_order(bboxes):\n",
    "    \"\"\"Change bounding box encoding order.\n",
    "\n",
    "    Tensorflow works with the (y_min, x_min, y_max, x_max) order while we work\n",
    "    with the (x_min, y_min, x_max, y_min).\n",
    "\n",
    "    While both encoding options have its advantages and disadvantages we\n",
    "    decided to use the (x_min, y_min, x_max, y_min), forcing us to switch to\n",
    "    Tensorflow's every time we want to use function that handles bounding\n",
    "    boxes.\n",
    "\n",
    "    Arguments:\n",
    "        bboxes: A Tensor of shape (total_bboxes, 4).\n",
    "\n",
    "    Returns:\n",
    "        bboxes: A Tensor of shape (total_bboxes, 4) with the order swaped.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: see `tf.unstack`, `tf.stack`.\n",
    "\n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def apply_nms(proposals, scores):\n",
    "    \"\"\"Applies non-maximum suppression to proposals.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals: Tensor of shape (num_proposals, 4), holding the\n",
    "            coordinates of the proposals' bounding boxes.\n",
    "        scores: Tensor of shape (num_proposals,), holding the\n",
    "            scores associated to each bounding box.\n",
    "        \n",
    "    Returns:\n",
    "        (`proposals`, `scores`), but with NMS applied, and ordered by score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: See `tf.image.non_max_suppression` to perform NMS, our `change_order`\n",
    "    # to prepare the bounding boxes, and `tf.gather` to pick indices out of a\n",
    "    # Tensor.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return proposals, scores\n",
    "\n",
    "pre_merge_proposals, pre_merge_scores = proposals, scores\n",
    "proposals, scores = apply_nms(proposals, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn: what have we detected? Play around with NMS.\n",
    "\n",
    "Let's take a look at our current results, so we can understand what we have to work with.\n",
    "\n",
    "1. How is it different than before?\n",
    "2. Is the importance of something like NMS more clear now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first `topn` proposals, as ordered by score.\n",
    "@interact(\n",
    "    nms=Checkbox(value=True, description='Apply NMS'),\n",
    "    topn=pager(200, 1, min=1, value=10, description='Number of proposals')\n",
    ")\n",
    "def draw(nms, topn):\n",
    "    if nms:\n",
    "        p = proposals\n",
    "        s = scores\n",
    "    else:\n",
    "        p = pre_merge_proposals\n",
    "        s = pre_merge_scores\n",
    "        \n",
    "    print('Minimum score: {:.2f}'.format(s[topn]))\n",
    "    return draw_bboxes(image, p[:topn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the center positions have changed pre- and post- merging of proposals when restricted to the first $2000$ proposals. After applying NMS, we should have improved our coverage of the image somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = tf.nn.top_k(pre_merge_scores, k=proposals.shape[0])\n",
    "props = tf.gather(pre_merge_proposals, top_k.indices).numpy()\n",
    "\n",
    "pre_merge_centers = np.stack([\n",
    "    (props[:, 0] + props[:, 2]) / 2,\n",
    "    (props[:, 1] + props[:, 3]) / 2,\n",
    "], axis=1)\n",
    "\n",
    "post_merge_centers = np.stack([\n",
    "    (proposals[:, 0] + proposals[:, 2]) / 2,\n",
    "    (proposals[:, 1] + proposals[:, 3]) / 2,\n",
    "], axis=1)\n",
    "\n",
    "_, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "axes[0][0].set_title('x-axis center positions pre-merge')\n",
    "axes[0][0].hist(pre_merge_centers[:, 0], bins=40)\n",
    "axes[0][1].set_title('y-axis center positions pre-merge')\n",
    "axes[0][1].hist(pre_merge_centers[:, 1], bins=40)\n",
    "axes[1][0].set_title('x-axis center positions post-merge')\n",
    "axes[1][0].hist(post_merge_centers[:, 0], bins=40)\n",
    "axes[1][1].set_title('y-axis center positions post-merge')\n",
    "axes[1][1].hist(post_merge_centers[:, 1], bins=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN Conclusions\n",
    "\n",
    "This concludes the work on the Region Proposal Network! We now have a mechanism to, given an image of arbitrary size, return **regions of interest** (i.e. proposals), where it looks like an object is present.\n",
    "\n",
    "Having gone through all the steps, from generating anchors around the image to predicting and filtering proposals, we now have a list of `POST_NMS_TOP_N` proposals (two thousand, in this case), each with an objectness score assigned.\n",
    "\n",
    "Two thousand proposals are, of course, many more than what we need. Also, we need to assign an actual class to each of these proposals, or discard them if they're not correct. That will be attacked by the rest of our object detection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Standardizing proposals: Region of Interest Pooling\n",
    "\n",
    "So far, we have obtained regions of interest for an arbitrarily-sized input image. Thousands of them. And all of them of a different size. As you've probably seen in the last visualization, some of them may be very small while others very big.\n",
    "\n",
    "The objective of this stage is twofold:\n",
    "1. Get the proposals, defined in **pixel-space coordinates**, back to the **feature maps**.\n",
    "2. Get them all into a **fixed size** so they can later be fed into a fully-connected neural network.\n",
    "\n",
    "This final size of each region of interest will be $7\\times7\\times1024$.\n",
    "> * $1024$ is the number of filters that our feature map has.\n",
    "> * $7\\times7$ corresponds to the common spatial size all proposals will have.\n",
    ">\n",
    "> This implies, as you might notice, that the aspect ratio of the proposals will change.\n",
    "\n",
    "On the original Faster R-CNN paper, a technique called RoI pooling is used. Here, instead, we use the `tf.image.crop_and_resize` Tensorflow function, which is (in performance terms) almost equivalent but simpler to implement.\n",
    "\n",
    "Also, bear in mind that the RoI pooling layer **first** resizes to *double* of the pooling size (i.e. gets regions of $14\\times14$) and then uses max pooling to get the final $7\\times7$ regions. This makes the resulting regions more smooth and makes them capture more details. One could even go further and resize to $28\\times28$ or more, but since we're **making a copy** of the feature map, memory usage will rapidly go up (trade-offs...).\n",
    "\n",
    "So much for an introduction. The implementation should be relatively straightforward. So go on ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task: implement `roi_pooling` function and the `normalize_bboxes` helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_bboxes(proposals, im_shape):\n",
    "    \"\"\"\n",
    "    Gets normalized coordinates for RoIs (between 0 and 1 for cropping)\n",
    "    in TensorFlow's order (y1, x1, y2, x2).\n",
    "\n",
    "    Arguments:\n",
    "        roi_proposals: A Tensor with the bounding boxes of shape\n",
    "            (total_proposals, 4), where the values for each proposal are\n",
    "            (x_min, y_min, x_max, y_max).\n",
    "        im_shape: A Tensor with the shape of the image (height, width).\n",
    "\n",
    "    Returns:\n",
    "        bboxes: A Tensor with normalized bounding boxes in TensorFlow's\n",
    "            format order. Its should is (total_proposals, 4).\n",
    "    \"\"\"\n",
    "    \n",
    "    # See `tf.unstack`, `tf.stack`, `tf.cast`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "\n",
    "def roi_pooling(feature_map, proposals, im_shape, pool_size=7):\n",
    "    \"\"\"Perform RoI pooling.\n",
    "\n",
    "    This is a simplified method than what's done in the paper that obtains\n",
    "    similar results. We crop the proposal over the feature map and resize it\n",
    "    bilinearly.\n",
    "    \n",
    "    This function first resizes to *double* of `pool_size` (i.e. gets\n",
    "    regions of (pool_size * 2, pool_size * 2)) and then uses max pooling to\n",
    "    get the final `(pool_size, pool_size)` regions.\n",
    "    \n",
    "    Arguments:\n",
    "        feature_map: Tensor of shape (1, W, H, C), with WxH the spatial\n",
    "            shape of the feature map and C the number of channels (1024\n",
    "            in this case).\n",
    "        proposals: Tensor of shape (total_proposals, 4), holding the proposals\n",
    "            to perform RoI pooling on.\n",
    "        im_shape: A Tensor with the shape of the image (height, width).\n",
    "        pool_size (int): Final width/height of the pooled region.\n",
    "    \n",
    "    Returns:\n",
    "        Pooled feature map, with shape `(num_proposals, pool_size, pool_size,\n",
    "        feature_map_channels)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: See `tf.image.crop_and_resize` to get crops out of the feature map\n",
    "    # and resize them.\n",
    "    \n",
    "    # Tip: You can **ignore the `box_ind` argument** by passing an array of the\n",
    "    # correct size filled with zeros (one per proposal). This is because we are\n",
    "    # using batch size of one.\n",
    "    \n",
    "    # Tip: Remember to resize to `2 * pool_size` first.\n",
    "    \n",
    "    # Tip: Remember to perform the max pooling as described above, by using\n",
    "    # the `tf.nn.max_pool` function.\n",
    "    \n",
    "    # N.B.: You can resize to `(pool_size, pool_size)` directly and avoid the\n",
    "    # max pooling step, though the results *will* be inferior.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return pooled\n",
    "\n",
    "\n",
    "pooled = roi_pooling(feature_map, proposals, (image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to gain an intuition on what exactly is being done here, let's now visualize our **pooled regions of interest**, along with the image patches they come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = pooled.numpy()\n",
    "\n",
    "# Pool the images too to visualize, but using a higher pooling size so\n",
    "# we don't lose too much resolution.\n",
    "image_crops = roi_pooling(\n",
    "    image, proposals,\n",
    "    (image.shape[1], image.shape[2]),\n",
    "    pool_size=140\n",
    ").numpy().astype(np.uint8)\n",
    "\n",
    "\n",
    "@interact(\n",
    "    fm_idx=pager(pool.shape[-1], 1, 'Feature map index'),\n",
    "    im_idx=pager(pool.shape[0], 25, 'Proposals')\n",
    ")\n",
    "def display_pooled_proposal(fm_idx=0, im_idx=0):\n",
    "    axes = image_grid(25, 5, sizes=(3, 3))\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if im_idx * 25 + idx >= pool.shape[0]:\n",
    "            break\n",
    "            \n",
    "        fm = (\n",
    "            pool[idx, :, :, fm_idx]\n",
    "            / pool[idx, :, :, fm_idx].max()\n",
    "            * 255\n",
    "        ).astype(np.uint8)\n",
    "        \n",
    "        # Get the pooled image regions.\n",
    "        img = image_crops[im_idx * 25 + idx, ...]\n",
    "        \n",
    "        fm_image = Image.fromarray(fm, mode='L').convert('RGBA')\n",
    "        fm_image = fm_image.resize(img.shape[0:2][::-1], resample=Image.NEAREST)\n",
    "\n",
    "        # Add some alpha to overlay it over the image.\n",
    "        fm_image.putalpha(120)\n",
    "\n",
    "        base_image = Image.fromarray(img)\n",
    "        base_image.paste(fm_image, (0, 0), fm_image)\n",
    "        \n",
    "        ax.imshow(base_image, aspect='auto')\n",
    "\n",
    "    plt.subplots_adjust(wspace=.02, hspace=.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using the proposals: Region-CNN\n",
    "\n",
    "We're ready for the final stage! Here we'll be doing two things:\n",
    "* Running our set of fixed-sized proposals through a network akin to what was done in the RPN: one input, two outputs. In this case, instead of an objectness score, the output will be a class score (plus a possible **background** score).\n",
    "* Get these thousands of proposals into a reasonable number. We'll be performing NMS again, but this time per class.\n",
    "\n",
    "As you can see, it looks like more of the same, which (save some details) it effectively is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're finally ready to perform the classification, so load the class names.\n",
    "with open('checkpoint/classes.json') as f:\n",
    "    classes = json.load(f)\n",
    "    \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classification network\n",
    "\n",
    "As we mentioned before, this last stage will get the proposals through a fully-connected layer. However, before doing that, we'll perform a bit more feature extraction.\n",
    "\n",
    "You might remember when you were implementing the RPN that we used the first three out of four blocks of the ResNet for feature extraction, discarding the final block. The reasoning behind this move was to leverage the fact that the block three should, in principle, detect more *abstract features* than the final, block four. Now we're ready to perform the final classification, so we will first pass our proposals (which are cuts, although resized, of the original feature map) through **the block four of the ResNet**.\n",
    "\n",
    "Also, once we do this, since we've already extracted all the features we care about, we'll perform **Global Average Pooling** which means, essentialy, to average out the spatial information: we only care that in a given proposal, some feature was present all around. That means we'll be left with a single vector per proposal.\n",
    "\n",
    "And, finally, pass this fixed-length vector through two fully-connected layers: one for the bounding box resizings and one for the classes. Since we've used the block four of the ResNet already, we'll not be using an intermediate layer.\n",
    "\n",
    "> ### Dimensions sanity check helper\n",
    "> In the case of using 2000 proposals and the ResNet as we did before:\n",
    ">\n",
    "> 1. `proposals` should be of shape $(2000, 7, 7, 1024)$.\n",
    "> 2. After running through ResNet tail, we should get something of shape $(2000, 7, 7, 2048)$.\n",
    "> 3. After Global Average Pooling, we should condense the spatial dimensions and get $(2000, 2048)$.\n",
    "> 4. Class scores should be $(2000, 81)$ (80 classes + background).\n",
    "> 5. Box regression scores should be $(2000, 320)$ ($80 \\times 4$ since we have 4 coordinates for each box).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_resnet_tail.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming task: implement `run_rcnn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_rcnn(pooled, num_classes):\n",
    "    \"\"\"Run the RCNN layers through the pooled features.\n",
    "\n",
    "    This directly applies a fully-connected layer from `features`\n",
    "    to the two outputs we want: a class probability (plus the\n",
    "    background class) and the bounding box resizings (one per\n",
    "    class).\n",
    "    \n",
    "    In order to obtain the class probability, we apply a softmax\n",
    "    over the scores obtained from the dense layer, similar to the RPN.\n",
    "    \n",
    "    Arguments:\n",
    "        pooled: Pooled feature map, with shape `(num_proposals,\n",
    "            pool_size, pool_size, feature_map_channels)`.\n",
    "        num_classes: Number of classes for the R-CNN.\n",
    "            \n",
    "    Returns:\n",
    "        Tuple of Tensors, with the first being the output of the\n",
    "        bbox resizings `(W * H * proposals, 4)` and the second being\n",
    "        the class scores, of size `(pool_size ^ 2 * proposals,\n",
    "        num_classes)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remember, you need to do three things with `pooled`:\n",
    "    # * Pass them through the ResNet block four.\n",
    "    #   (Tip: See the function `run_resnet_tail`s docstring above.)\n",
    "    # * Perform Global Average Pooling.\n",
    "    #   (Tip: See the function `tf.reduce_mean`.)\n",
    "    # * Run them through two fully-connected layers.\n",
    "    #   (Tip: See the functions `tf.layers.dense`, `tf.nn.softmax`.)\n",
    "    \n",
    "    # W.r.t the fully-connected layers, remember:\n",
    "    # * To add an extra class for the background class.\n",
    "    # * To have bounding-box resizings **per-class**.\n",
    "    \n",
    "    # The names of the layers should be: `rcnn/fc_classifier` for\n",
    "    # the classification head, and `rcnn/fc_bbox` for the bbox\n",
    "    # resizing head.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return rcnn_bbox, rcnn_cls_prob\n",
    "\n",
    "\n",
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    bbox_pred, cls_prob = run_rcnn(pooled, len(classes))\n",
    "    \n",
    "    \n",
    "assert bbox_pred.shape[0] == pooled.shape[0], 'Number of proposals should match'\n",
    "assert cls_prob.shape[0] == pooled.shape[0], 'Number of proposals should match'\n",
    "\n",
    "assert bbox_pred.shape[1] == len(classes) * 4, 'There should be 4 bbox resizings per class'\n",
    "assert cls_prob.shape[1] == len(classes) + 1, 'There should be 81 class probabilities (remember the background!)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn: play around with proposals and their corresponding class determined by R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results now, and see whether they make sense.\n",
    "\n",
    "We'll display the **most probable class for each proposal**, before applying the final class-specific resizing: this is the pooled region of interest, what the classifier actually looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = ['background'] + classes\n",
    "    \n",
    "preds = np.argmax(cls_prob.numpy(), axis=1)\n",
    "\n",
    "@interact(page=pager(len(preds), 20, 'Proposals'))\n",
    "def display_predictions(page):\n",
    "    axes = image_grid(20, 5, sizes=(3, 3))\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if 20 * page + idx >= image_crops.shape[0]:\n",
    "            break\n",
    "            \n",
    "        ax.imshow(image_crops[20 * page + idx, ...], aspect='auto')\n",
    "        ax.set_title(output_classes[preds[20 * page + idx]])\n",
    "\n",
    "    plt.subplots_adjust(wspace=.02, hspace=.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn: play around with bounding box resizings (object \"refinements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now display the proposals again, but with the **bounding box resizings applied**.\n",
    "\n",
    "Corrections are done **per-class**. In order to understand how much these predictions vary, we take some proposals and apply the different possible resizings.\n",
    "\n",
    "For each region, we first display the resizing for the most probable class and then for three other random classes. If the most probable class is background, we ignore it.\n",
    "\n",
    "Do you notice anything in particular? Which resizing is the one that fits better to the detected object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target normalization variances to adjust the output of the R-CNN so it trains better.\n",
    "TARGET_VARIANCES = np.array([0.1, 0.1, 0.2, 0.2], dtype=np.float32)\n",
    "\n",
    "# We only consider proposals for which the most-probable class was non-background.\n",
    "preds = np.argmax(cls_prob.numpy(), axis=1)\n",
    "non_background = (preds != 0)\n",
    "\n",
    "non_bg_preds = preds[non_background]\n",
    "non_bg_proposals = proposals.numpy()[non_background]\n",
    "non_bg_bboxes = bbox_pred.numpy()[non_background]\n",
    "non_bg_count = len(np.flatnonzero(non_background))\n",
    "\n",
    "\n",
    "@interact(page=pager(non_bg_count, 3, 'Proposals'))\n",
    "def display_resizings(page):\n",
    "    _, axes = plt.subplots(3, 5, figsize=(16, 10))\n",
    "    \n",
    "    for row_idx, cols in enumerate(axes):\n",
    "        for col in cols:\n",
    "            col.axis('off')\n",
    "\n",
    "        proposal_idx = 3 * page + row_idx    \n",
    "        if proposal_idx >= non_bg_count:\n",
    "            continue\n",
    "        \n",
    "        # Original region.\n",
    "        # (Using original region size so comparison is easier to the eye.)\n",
    "        x_min, y_min, x_max, y_max = clip_boxes(\n",
    "            non_bg_proposals[proposal_idx:proposal_idx + 1],\n",
    "            image.shape[1:3]\n",
    "        )[0].numpy().astype(np.int)\n",
    "        cols[0].imshow(image[0, y_min:y_max, x_min:x_max, :])\n",
    "        cols[0].set_title('Region')\n",
    "        \n",
    "        # Per-class region, correct class first.\n",
    "        class_ids = np.concatenate([\n",
    "            np.array([non_bg_preds[proposal_idx] - 1]),\n",
    "            np.random.randint(0, len(classes), 3)\n",
    "        ])\n",
    "        for col, class_id in zip(cols[1:], class_ids):\n",
    "            cls_bbox_pred = non_bg_bboxes[\n",
    "                proposal_idx:proposal_idx + 1,\n",
    "                (4 * class_id):(4 * class_id + 4)\n",
    "            ]\n",
    "\n",
    "            cls_objects = decode(\n",
    "                non_bg_proposals[proposal_idx:proposal_idx+1],\n",
    "                cls_bbox_pred * TARGET_VARIANCES\n",
    "            ).numpy()\n",
    "            \n",
    "            x_min, y_min, x_max, y_max = clip_boxes(\n",
    "                cls_objects, image.shape[1:3]\n",
    "            )[0].numpy().astype(np.int)\n",
    "\n",
    "            col.imshow(image[0, y_min:y_max, x_min:x_max, :])\n",
    "            col.set_title(classes[class_id])\n",
    "        \n",
    "    plt.subplots_adjust(wspace=.02, hspace=.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the object candidates\n",
    "\n",
    "We're finally getting there! We have one last step to do: getting the final predictions.\n",
    "\n",
    "What we have now is a list of `POST_NMS_TOP_N` proposals (around $2000$), each with 81 class scores (80 classes plus the background) and 80 bounding box resizings. Out of this, we'll have a total of $2000 \\times 80 = 160000$ candidate objects, each with a **score** (its class score) and **bounding box resizing**. We do this in order to consider **all** possible object classifications for a given region proposal: if the most-probable class of a bounding box has a score of $0.48$ and the second one has a score of $0.47$, it is important to consider **both** variants, and not just the highest-scored one.\n",
    "\n",
    "Of course, we'll not really build the $160000$ proposals at once, but instead perform NMS **on a class-by-class basis**, keeping only the top $100$ proposals per class. Then we'll order all the proposals and keep only the top $300$, which will be the output of our algorithm.\n",
    "\n",
    "**We've already implemented all this part for you**, as it's same as above but on a class-by-class basis. With a little work, you should be able to do it by yourself if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objects, labels, probs = rcnn_proposals(\n",
    "    proposals, bbox_pred, cls_prob, image.shape[1:3], 80,\n",
    "    min_prob_threshold=0.0,\n",
    ")\n",
    "\n",
    "objects = objects.numpy()\n",
    "labels = labels.numpy()\n",
    "probs = probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of detections above 0.1 probability:', len(labels[probs > 0.1]))\n",
    "print('Number of detections above 0.5 probability:', len(labels[probs > 0.5]))\n",
    "print('Number of detections above 0.7 probability:', len(labels[probs > 0.7]))\n",
    "print()\n",
    "\n",
    "# Accumluated probability score per class.\n",
    "probs_per_class = np.bincount(labels, weights=probs, minlength=len(classes))\n",
    "top_n = probs_per_class.argsort()[::-1][:5]\n",
    "print('Top 5 predicted classes:')\n",
    "for cls_idx in top_n:\n",
    "    print('   {}: {} ({:.2f})'.format(cls_idx, classes[cls_idx], probs_per_class[cls_idx]))\n",
    "    \n",
    "_, ax = plt.subplots(1, figsize=(16, 4))\n",
    "ax.bar(np.arange(80), probs_per_class)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, however, we don't want $300$ low-quality detections, but whatever is good. What we do in practice is to filter detections by their probability score (which is the candidate object's class score).\n",
    "\n",
    "Let's take a look at the $300$ candidate objects and see how much the predictions change when filtering by score. In the real world, you'll probably use a threshold above $0.5$, depending on your desired precision vs. recall trade-off (too high a threshold will miss detections, while too low will add noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider = FloatSlider(\n",
    "    min=0.0, max=1.0, step=0.01, value=0.7,\n",
    "    description='Probability threshold',\n",
    "    layout=Layout(width='600px'),\n",
    "    style={'description_width': 'initial'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "@interact(prob=slider)\n",
    "def display_objects(prob):\n",
    "    MAX_TO_DRAW = 50\n",
    "\n",
    "    mask = probs > prob\n",
    "\n",
    "    return draw_bboxes_with_labels(\n",
    "        image, classes,\n",
    "        objects[mask][:MAX_TO_DRAW],\n",
    "        labels[mask][:MAX_TO_DRAW],\n",
    "        probs[mask][:MAX_TO_DRAW],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summing up\n",
    "\n",
    "**Congratulations!** You finished your own implementation of Faster R-CNN, one of the state-of-the-art object detection algorithms.\n",
    "\n",
    "Throughout this notebook you should have learned quite a few things:\n",
    "* **How modern object detectors work**: what inputs they take, what kinds of operations and logic they do, and how much control we have in their workings.\n",
    "* In particular, **how Faster R-CNN works**, very much in depth.\n",
    "* **How to use Tensorflow and numpy in the context of computer vision and object detection**. Going a bit more than the usual \"stack three layers and call it a day\": we've worked with arbitrarily-sized inputs, used conditionals, filtering and other non-standard functions, all within the Tensorflow graph (meaning it can run entirely within a GPU).\n",
    "* **How to visualize the inner workings of an object detection pipeline**. By leveraging an already-trained network, we could see and corroborate each step of the pipeline to understand what goes behind the scenes and whether we made any errors. This process may have also given you some clues in how to improve the algorithm itself.\n",
    "\n",
    "This, of course, is just the beginning. Some things you could try, going forward, are:\n",
    "* **Implement the training of a Faster R-CNN model**. We barely touched on this part, using a pre-trained checkpoint provided by us. The training, apart from using the autograd of your favorite deep learning library, requires some extra steps:\n",
    "\n",
    "    * Implement the **targets**. We're using supervised learning to train this, so how does training data fit into this? We need to train both the RPN and the R-CNN. In order to do this, we need to build mini-batches of training data for both components, by matching ground-truth boxes to our proposals. You can learn how this is done in our implementation of [Luminoth](https://github.com/tryolabs/luminoth).\n",
    "\n",
    "    * Implement the **loss functions**. Once the targets are in place, we need to select good losses and balance our dataset in order to train correctly. There may be some difficulties when training, as we are optimizing four losses in total (two for the RPN, two for the R-CNN).\n",
    "\n",
    "* Improve the algorithm itself. Faster-RCNN has been out for a while now, and while it's still very competitive, there are some known improvements to do. For instance, the RPN can be replaced entirely with the **Feature Pyramid Network** (FPN) [1] and the loss exchanged with the **Focal Loss** [2], to obtain the algorithm called **RetinaNet**.\n",
    "\n",
    "\n",
    "* [1] https://arxiv.org/pdf/1612.03144.pdf\n",
    "* [2] https://arxiv.org/pdf/1708.02002.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
