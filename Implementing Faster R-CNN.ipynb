{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Faster R-CNN\n",
    "\n",
    "The objective of this activity is to implement the main parts of the Faster R-CNN algorithm. We'll use pre-trained weights to guide and facilitate the process, and implement all the stages one by one.\n",
    "\n",
    "We've tried to keep code in the notebooks to a minimum, mainly data manipulation and visualization, to make it easy enough to follow. All accompanying code is under the `workshop` Python package.\n",
    "\n",
    "After some introductory code, the notebook will continue as follows:\n",
    "* Playing with a **pre-trained Resnet** to obtain features out of an image.\n",
    "* Generate regions of interest by implementing the **Region Proposal Network** detailed in [1].\n",
    "* Prepare this regions to be fed to the second stage, by applying **RoI pooling**.\n",
    "* Classify and refine said networks by passing them through an **R-CNN**, as detailed in [2].\n",
    "\n",
    "We'll present you with stubs for the different functions required and your task will be to fill them in.\n",
    "\n",
    "Note that, due to time constraints, we won't actually implement anything relating to training the model itself. We'll be focusing on the required machinery for inference, and give some pointers on what's missing to train it from scratch.\n",
    "\n",
    "* [1] Ren, Shaoqing, et al. *Faster R-CNN: Towards real-time object detection with region proposal networks.*\n",
    "* [2] Girshick, Ross. *Fast R-CNN.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The basics\n",
    "We'll start with some imports.\n",
    "\n",
    "The local imports are under the `workshop` package, which you should have installed using `pip install -e workshop/` in the virtualenv you're running your notebook on. [Remove if we're going to use Azure.]\n",
    "\n",
    "Within `workshop` we have some modules:\n",
    "* `vis`: various visualization utilities to draw bounding boxes, sliders, etc.\n",
    "* `image`: utilities for reading images and loading them into PIL (the imaging library).\n",
    "* `resnet`: the implementation for the base network we're going to use (more on this shortly).\n",
    "* `faster`: utilities and parts we won't be implementing but provide for completeness' sake.\n",
    "\n",
    "Let's test some things to make sure everything is up and ready to go.\n",
    "\n",
    "Start by running the following in your terminal, and then test the rest of the imports:\n",
    "```bash\n",
    " $ jupyter nbextension enable --py widgetsnbextension\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Checkbox, FloatSlider, Layout\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Try to enable TF eager execution, or do nothing if running again.\n",
    "try:\n",
    "    tf.enable_eager_execution()\n",
    "except ValueError:\n",
    "    # Already executed.\n",
    "    pass\n",
    "\n",
    "\n",
    "# Local imports.\n",
    "from workshop.faster import clip_boxes, rcnn_proposals, run_base_network, run_resnet_tail, sort_anchors\n",
    "from workshop.image import open_all_images, open_image, to_image\n",
    "from workshop.vis import add_rectangle, draw_bboxes, draw_bboxes_with_labels, image_grid, pager\n",
    "\n",
    "# Notebook-specific settings.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now load some images to play with and display them below. Change which image is passed to the `to_image` function to see it in full size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = open_all_images('images/')\n",
    "\n",
    "axes = image_grid(len(images))\n",
    "for ax, (name, image) in zip(axes, images.items()):\n",
    "    ax.imshow(np.squeeze(image))\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.subplots_adjust(wspace=.01)\n",
    "plt.show()\n",
    "\n",
    "image = images['woman']\n",
    "\n",
    "# `to_image` turns a `numpy.ndarray` into a PIL image, so it's displayed by the notebook.\n",
    "to_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# The base network: ResNet\n",
    "\n",
    "\n",
    "The basis for the Faster R-CNN algorithm is to leverage a pre-trained classifier network to extract feature maps (also called *activation maps*) from the image. For this implementation, we'll be using the popular ResNet 101 [3].\n",
    "\n",
    "We provide the implementation itself (which you can see in the `workshop.resnet` module), as well as a checkpoint with the weights (in the `checkpoint/` directory).\n",
    "\n",
    "---\n",
    "\n",
    "### Aside\n",
    "\n",
    "The ResNet architecture consists of four stacked **blocks**, after which a fully-connected layer is attached. As is expected of CNNs, these blocks detect features from most simple to most complex. For this part of the algorithm, we're using the output of the **block 3**, so we get somewhat generic features. The intuition is that, if we go all the way and use block 4, we might have things that are too specific to the dataset used to pre-train the ResNet (the Imagenet dataset) and thus not as desirable for a network that wants to identify generic objects. \n",
    "\n",
    "---\n",
    "\n",
    "Run the base network on different images, in order to see how the different feature maps behave. **Can you notice any particular features being detected in the activation maps?**\n",
    "\n",
    "* [3] He, Kaiming, et al. *Deep residual learning for image recognition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    feature_map = run_base_network(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact(page=pager(1024, 20, 'Feature map'))\n",
    "def display_feature_maps(page):\n",
    "    axes = image_grid(20)\n",
    "    for idx, ax in enumerate(axes):\n",
    "        if page * 20 + idx >= 1024:\n",
    "            break\n",
    "        ax.imshow(\n",
    "            feature_map.numpy()[0, :, :, page * 20 + idx],\n",
    "            cmap='gray', aspect='auto'\n",
    "        )\n",
    "\n",
    "    plt.subplots_adjust(wspace=.01, hspace=.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overlay the feature maps into the image themselves, so we can take a more detailed look into what feature of the image the ResNet reacts to.\n",
    "\n",
    "See, for example:\n",
    "* Feature maps 19, 22 in `cats`.\n",
    "* Feature map 34, 64 in `bicycles`.\n",
    "* Feature map 253 in `kids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(idx=pager(1024, 1, 'Feature map index'))\n",
    "def overlay_feature_map(idx):\n",
    "    # Normalize the feature map so we get the whole range of colors.\n",
    "    fm = (\n",
    "        feature_map.numpy()[0, :, :, idx]\n",
    "        / feature_map.numpy()[0, :, :, idx].max()\n",
    "        * 255\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    # Resize the feature map without interpolation.\n",
    "    fm_image = Image.fromarray(fm, mode='L').convert('RGBA')\n",
    "    fm_image = fm_image.resize(image.shape[1:3][::-1], resample=Image.NEAREST)\n",
    "    \n",
    "    # Add some alpha to overlay it over the image.\n",
    "    fm_image.putalpha(200)\n",
    "    \n",
    "    base_image = to_image(image)\n",
    "    base_image.paste(fm_image, (0, 0), fm_image)\n",
    "    \n",
    "    return base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section didn't require any implementation at all, but get ready, because we're about to. The main idea here was illustrating what we mean when we say that the later layers of a classification network are **feature detectors**, reacting to particular structures in an image.\n",
    "\n",
    "What would you do if you were to use this information to detect objects? How could you leverage the fact that we can say \"there's a cat ear here!\"? We're going to explore these questions in the following sections.\n",
    "\n",
    "For now, back to the slides!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Finding stuff with the Region Proposal Network\n",
    "\n",
    "Having gone through the theory, we'll now turn our attention to implementing a **Region Proposal Network**. The idea, as we've seen, is to use the feature maps provided by the ResNet to find out **where** there might be an object located.\n",
    "\n",
    "This is where **anchors** come into play. We'll take a grid of points over the image and consider several (15, in this case) anchors, or reference boxes, for each of them. The RPN layers themselves will then predict whether there's an object in each of these 15 boxes **and** how much we need to resize them to better fit it.\n",
    "\n",
    "The tasks we have ahead of us are, thus:\n",
    "* Get the **coordinates** $(x_{min}, y_{min}, x_{max}, y_{max})$ for each of the anchors. There are $15$ anchors and the centers will be separated by approximately $16$ pixels, so we're talking about several thousand of coordinates.\n",
    "* Find out how to do the special **encoding and decoding** of coordinates described in Faster R-CNN so the RPN can predict locations in the image.\n",
    "* Build the **convolutional layers** comprising the RPN and run them through different images.\n",
    "* **Translate the predictions** of the RPN layer into usable proposals.\n",
    "\n",
    "**Note**: Except in specific cases, we'll be using the convention $(x_{min}, y_{min}, x_{max}, y_{max})$ to denote a bounding box, were $(x_{min}, y_{min})$ corresponds to the top-left point and $(x_{max}, y_{max})$ the bottom right. As usual with image processing, the origin of the coordinate system, $(0, 0)$, is on the top-left of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating anchors\n",
    "\n",
    "We'll get the anchor's coordinates in two steps. First we'll implement the `generate_anchors_reference` function which will return, given the anchors' settings (i.e. size, aspect ratio, scales), the coordinates for said boxes (in pixel space) assuming they're centered around (0, 0). This will give us, effectively, a $(15, 4)$ array.\n",
    "\n",
    "Then, we'll sum those coordinates to each of the **anchor centers** of the image in the function `generate_anchors`. Given that we're using a ResNet 101, which has a downsampling factor of 16 (i.e. every point in the final feature map corresponds to a $16\\times16$ region of the original image), we'll select the centers every 16 pixels in each direction.\n",
    "\n",
    "Go on and implement `generate_anchors_reference`, and check that the output makes sense. You can try varying the anchor settings to see if it still makes sense. There are three settings for the anchors:\n",
    "* `base_size`: **side length for a square anchor**, in pixels (e.g. 256). Increasing it makes the reference box cover more area of the image.\n",
    "* `scales`: **scale factors** to consider taking `base_size` as reference. For instance, a scale of `2` will make the effective size `512` if base size was `256`.\n",
    "* `aspect_ratios`: **aspect ratios** of the anchors, expressed as the value of `height / width`. Note that *changing the aspect ratio doesn't change the area the anchor covers*. An aspect ratio of `2` means that, for the area covered by a square anchor of of `base_size`, we should get a rectangle of twice the height than width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors_reference(base_size, aspect_ratios, scales):\n",
    "    \"\"\"Generate base set of anchors to be used as reference for all anchors.\n",
    "\n",
    "    Anchors vary only in width and height. Using the base_size and the\n",
    "    different ratios we can calculate the desired widths and heights.\n",
    "\n",
    "    Aspect ratios maintain the area of the anchors, while scales apply to the\n",
    "    length of it (and thus affect it squared).\n",
    "\n",
    "    Arguments:\n",
    "        base_size (int): Base size of the base anchor (square).\n",
    "        aspect_ratios: Ratios to use to generate different anchors. The ratio\n",
    "            is the value of height / width.\n",
    "        scales: Scaling ratios applied to length.\n",
    "\n",
    "    Returns:\n",
    "        anchors: Numpy array with shape (total_aspect_ratios * total_scales, 4)\n",
    "            with the corner points of the reference base anchors using the\n",
    "            convention (x_min, y_min, x_max, y_max).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: if you want to write vectorized code for this function, the\n",
    "    # `np.meshgrid` and `np.stack` functions might be useful.\n",
    "    \n",
    "    # Tip: For this function, it's not necessary to use Tensorflow, as it's\n",
    "    # used as a constant in the following function. Use whatever feels right\n",
    "    # for you.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "    \n",
    "    # We sort the anchors to the value expected by our pre-trained network.\n",
    "    return sort_anchors(anchors)\n",
    "\n",
    "\n",
    "references = generate_anchors_reference(\n",
    "    256,  # Base size.\n",
    "    [0.5, 1, 2],  # Aspect ratios.\n",
    "    [0.125, 0.25, 0.5, 1, 2],  # Scales.\n",
    ")\n",
    "\n",
    "print('Anchor references (real image size):')\n",
    "print()\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should have obtained 5 areas and 3 different aspect ratios in our\n",
    "# anchor references.\n",
    "widths = references[:, 2] - references[:, 0]\n",
    "heights = references[:, 3] - references[:, 1]\n",
    "\n",
    "aspect_ratios = np.round(heights / widths, 1)\n",
    "areas = widths * heights\n",
    "\n",
    "assert len(np.unique(areas)) == 5\n",
    "assert len(np.unique(aspect_ratios)) == 3\n",
    "\n",
    "print('Areas:', len(np.unique(areas)))\n",
    "print('Aspect ratios:', len(np.unique(aspect_ratios)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to check if the implementation is correct is just drawing them and making sure the result makes sense. If the image is too chaotic, you can try decreasing the number of scales and aspect ratios, but do revert it back before continuing.\n",
    "\n",
    "Remember: for scales, length is doubled, for aspect ratios, the area is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "ax.set_xlim([-500, 500])\n",
    "ax.set_ylim([-500, 500])\n",
    "\n",
    "for idx in range(references.shape[0]):\n",
    "    add_rectangle(ax, references[idx, :])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for reference, let's draw one over `image`, to see how they match. Since the anchor references are centered around $(0, 0)$, we can sum $P = (x_p, y_p)$ to get the references at $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = np.array([400, 270])\n",
    "\n",
    "# Sum the point on both the *_min and the *_max parts.\n",
    "references_at_point = references + np.concatenate([point, point])\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 20))\n",
    "ax.imshow(to_image(image))\n",
    "\n",
    "# Add a buffer around the image so we see the whole anchor references.\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "for idx in range(references_at_point.shape[0]):\n",
    "    add_rectangle(ax, references_at_point[idx, :])\n",
    "\n",
    "# Plot the reference point in use.\n",
    "ax.plot(point[0], point[1], marker='s', color='#dc3912', markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the larger boxes cover quite a bit of the image, while the smaller ones will be useful for detecting very small objects.\n",
    "\n",
    "Now, as we said before, we want the above for all **anchor centers**, which we said were going to be located every 16 pixels. For reference, the anchor centers are visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is actually defined within `run_base_network`, but for visualization\n",
    "# purposes, we're defining it again.\n",
    "OUTPUT_STRIDE = 16\n",
    "\n",
    "# Print the anchor centers in use.\n",
    "_, ax = plt.subplots(1, figsize=(16, 20))\n",
    "\n",
    "ax.imshow(to_image(image))\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "# We start from `output_stride / 2`, as we want the points centered around\n",
    "# the 16x16 region.\n",
    "for x in range(OUTPUT_STRIDE // 2, image.shape[2], OUTPUT_STRIDE):\n",
    "    for y in range(OUTPUT_STRIDE // 2, image.shape[1], OUTPUT_STRIDE):\n",
    "        ax.plot(x, y, marker='s', color='#dc3912', markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap up this part by getting the entire list of anchors for the image. This will be done within the `generate_anchors` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the anchor properties that will be used in our implementation.\n",
    "# Compared to the values picked on the paper, we've added two, smaller, scales\n",
    "# that help the model detect the smaller objects present in the COCO dataset.\n",
    "ANCHOR_BASE_SIZE = 256\n",
    "ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "ANCHOR_SCALES = [0.125, 0.25, 0.5, 1, 2]\n",
    "\n",
    "\n",
    "def generate_anchors(feature_map_shape):\n",
    "    \"\"\"Generate anchors for an image.\n",
    "\n",
    "    Using the feature map (the output of the pretrained network for an image)\n",
    "    and the anchor references (generated using the specified anchor sizes and\n",
    "    ratios), we generate a list of anchors.\n",
    "\n",
    "    Anchors are just fixed bounding boxes of different ratios and sizes that\n",
    "    are uniformly generated throught the image.\n",
    "\n",
    "    Arguments:\n",
    "        feature_map_shape: Shape of the convolutional feature map used as\n",
    "            input for the RPN. Should be (batch, height, width, depth).\n",
    "\n",
    "    Returns:\n",
    "        all_anchors: A Tensor with the anchors at every spatial of shape\n",
    "            `(feature_height, feature_width, num_anchors_per_points, 4)`\n",
    "            using the (x1, y1, x2, y2) convention.\n",
    "    \"\"\"\n",
    "\n",
    "    anchor_reference = generate_anchors_reference(\n",
    "        ANCHOR_BASE_SIZE, ANCHOR_RATIOS, ANCHOR_SCALES\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Tip: See `tf.meshgrid`, `tf.range`, `tf.expand_dims`, `tf.transpose`\n",
    "    # if doing it in a vectorized way.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "\n",
    "anchors = tf.reshape(generate_anchors(feature_map.shape), [-1, 4])\n",
    "\n",
    "print('Anchors (real image size):')\n",
    "print()\n",
    "print(anchors.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get three or four anchors to corroborate that the results makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the anchors. Try changing to different points of the image.\n",
    "# Note that we're referring to positions in the feature map here, so the\n",
    "# actual point in the image will be around `OUTPUT_STRIDE` times the value.\n",
    "point = np.array([30, 20])\n",
    "\n",
    "# Reshape back to (H, W, num_anchors, 4) so we can easily get a given point's anchors.\n",
    "anchors_at_point = anchors.numpy().reshape(\n",
    "    (feature_map.shape[1], feature_map.shape[2], 15, 4)\n",
    ")[point[1], point[0], :, :]\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 20))\n",
    "\n",
    "ax.imshow(to_image(image))\n",
    "ax.set_xlim([-100, image.shape[2] + 100])\n",
    "ax.set_ylim([image.shape[1] + 100, -100])\n",
    "\n",
    "for idx in range(anchors_at_point.shape[0]):\n",
    "    add_rectangle(ax, anchors_at_point[idx, :])\n",
    "\n",
    "# Plot the reference point in use.\n",
    "ax.plot(\n",
    "    point[0] * OUTPUT_STRIDE,\n",
    "    point[1] * OUTPUT_STRIDE,\n",
    "    marker='s', color='#dc3912', markersize=3\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we've finished generating the anchors that will be used by the RPN. This is, effectively, a list of $15 \\times F_x \\times F_y$, where $F_x, F_y$ are the feature map width and height, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and decoding coordinates\n",
    "\n",
    "\n",
    "Deep neural networks usually train and converge better when their outputs have zero mean and unit variance (and/or their intermediate values do so). Due to this, and the difficulty in predicting values in a possibly unbounded region (pixel coordinates), a special encoding is applied to the coordinates before passing them in to the network (and after getting them out).\n",
    "\n",
    "The idea behind the encoding is to express the coordinates of a bounding box $B$ as a set of four numbers $(D_x, D_y, D_w, D_h)$ (the **deltas**) and a reference bounding box $R$. $D_x$ and $D_y$ indicate how much the center of $B$ should be moved to reach the center of $R$, normalized by the size of $R$, while $D_w$ and $D_h$ indicate how much the width and height of $B$ must be increased or decreased to reach the size of $R$ (it's actually the log of that value, as you'll see below).\n",
    "\n",
    "For the following equations, we change from the $(x_{min}, y_{min}, x_{max}, y_{max})$ encoding to the center+dimensions encoding $(x, y, w, h)$, where $(x, y)$ are the **center coordinates**, and $(w, h)$ the **width and height**. The equations to encode $B = (x_b, y_b, w_b, h_b)$ with regard to $R = (x_r, y_r, w_r, h_r)$ are, then:\n",
    "\n",
    "$D_x = \\frac{x_b - x_r}{w_r} \\quad$\n",
    "$D_y = \\frac{y_b - y_r}{h_r} \\quad$\n",
    "$D_w = log \\frac{w_b}{w_r} \\quad$\n",
    "$D_h = log \\frac{h_b}{h_r}$\n",
    "\n",
    "The equations to decode $B = (x_b, y_b, w_b, h_b)$ given $R = (x_r, y_r, w_r, h_r)$ and deltas $D = (D_x, D_y, D_w, D_h)$ are:\n",
    "\n",
    "$x_b = D_x w_r + x_r \\quad$\n",
    "$y_b = D_y h_r + y_r \\quad$\n",
    "$w_b = e^{D_w} w_r \\quad$\n",
    "$h_b = e^{D_h} h_r \\quad$\n",
    "\n",
    "We'll implement two functions here, `encode` and `decode`. While only the latter will be used, it's useful to implement both in order to understand the whole process and to make it easier to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You might find it useful to implement the following function first in order\n",
    "# to obtain the dimensions (width and height) and center of a bounding box,\n",
    "# required for calculating the deltas in `encode` and `decode`.\n",
    "def get_dimensions_and_center(bboxes):\n",
    "    \"\"\"Obtain width, height and center coordinates of a bounding box.\n",
    "    \n",
    "    Arugments:\n",
    "        bboxes: Tensor of shape (num_bboxes, 4).\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of Tensors of shape (num_bboxes,) with the values\n",
    "        width, height, center_x and center_y corresponding to each\n",
    "        bounding box.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: Fully read the docstring above.\n",
    "    # Tip: You may find the Tensorflow function `tf.split` useful.\n",
    "\n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return width, height, ctx, cty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(references, bboxes):\n",
    "    \"\"\"Encode bounding boxes as deltas w.r.t. reference boxes.\n",
    "\n",
    "    Arguments:\n",
    "        references: Tensor of shape (num_bboxes, 4). With the same bbox\n",
    "            encoding.\n",
    "        bboxes: Tensor of shape (num_bboxes, 4). Having the bbox\n",
    "            encoding in the (x_min, y_min, x_max, y_max) order.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (num_bboxes, 4) with the different\n",
    "            deltas needed to transform the proposal to `references`. These\n",
    "            deltas are with regards to the center, width and height of the\n",
    "            two boxes.\n",
    "    \"\"\"\n",
    "   \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "# Encoding `bbox` with a `reference` having the same center should keep\n",
    "# the first two deltas at zero.\n",
    "ref = np.array([[0, 0, 100, 100]], dtype=np.float32)\n",
    "bbox = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "print('Same center:', encode(ref, bbox).numpy())\n",
    "\n",
    "# Encoding `bbox` with a `reference` having the same size should keep\n",
    "# the last two deltas at zero.\n",
    "ref = np.array([[0, 0, 100, 100]], dtype=np.float32)\n",
    "bbox = np.array([[50, 50, 150, 150]], dtype=np.float32)\n",
    "print('Same size:', encode(ref, bbox).numpy())\n",
    "\n",
    "# What other ways to check the functions can you think of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(references, deltas):\n",
    "    \"\"\"Decode proposals by applying deltas to bboxes.\n",
    "    \n",
    "    Arguments:\n",
    "        references: Tensor of shape (num_bboxes, 4). Having the bbox\n",
    "            encoding in the (x_min, y_min, x_max, y_max) order.\n",
    "        deltas: Tensor of shape (num_bboxes, 4). Deltas (as returned by\n",
    "            `encode`) that we want to apply to `bboxes`.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (num_bboxes, 4) with the decoded proposals,\n",
    "            obtained by applying `deltas` to `bboxes`.\n",
    "    \"\"\"\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "# Decoding `bbox` with zero `deltas` should keep the box as-is.\n",
    "bbox = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "delta = np.array([[0, 0, 0, 0]], dtype=np.float32)\n",
    "print('Zero delta:', decode(bbox, delta).numpy())\n",
    "\n",
    "# Decoding `bbox` with two ones at first then two zeros at `deltas`\n",
    "# should keep the size but move the box to the right one-width.\n",
    "bbox = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "delta = np.array([[1, 1, 0, 0]], dtype=np.float32)\n",
    "print('First-two are ones:', decode(bbox, delta).numpy())\n",
    "\n",
    "# Decoding `bbox` with two zeros at first then two ones at `deltas`\n",
    "# should enlarge the box while maintaining the center.\n",
    "bbox = np.array([[25, 25, 75, 75]], dtype=np.float32)\n",
    "delta = np.array([[0, 0, 1, 1]], dtype=np.float32)\n",
    "print('Last-two are ones:', decode(bbox, delta).numpy())\n",
    "\n",
    "# What other ways to check the functions can you think of? How can\n",
    "# you pick the deltas so that it exactly doubles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the round-trip of `encode` and `decode`, to see if they're consistent between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the round-trip: encode `bboxes` w.r.t. the references `refs`,\n",
    "# which gives us the deltas that transform `refs` into `bboxes`. Then\n",
    "# decode the `refs` with said deltas to see that, effectively, we\n",
    "# get `bboxes` back.\n",
    "refs = np.array([\n",
    "    [0, 0, 100, 100],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# You can try out other bounding boxes, just make sure to respect the\n",
    "# convention of first putting (x_min, y_min) then (x_max, y_max), or\n",
    "# you may get an invalid bounding box.\n",
    "bboxes = np.array([\n",
    "    [25, 25, 75, 75],\n",
    "    [10, -205, 120, 20],\n",
    "    [-35, 37, 38, 100],\n",
    "    [-0.2, -0.2, 0.2, 0.2],\n",
    "    [-25, -50, -5, -20],\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(\n",
    "    'Round-trip looks good:',\n",
    "    np.sum(np.abs(\n",
    "        decode(refs, encode(refs, bboxes)) - bboxes\n",
    "    )) < 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time left at the end, you could try to gain further intuition on what they do and what the encoding's edge cases and limitations are by looking at more examples and plotting the deltas as `bboxes` moves through the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layers\n",
    "\n",
    "We now have a variable-size feature map (with a factor of 16 times smaller than the original image) and we want to predict, for each spatial position, how to modify (i.e. the $4$ values from above, $D_{x, y, w, h}$) each of the $k = 15$ anchors. In this context, it makes sense, then, to use a convolutional layer (or more) on the feature map, where the final number of filters will be $4 \\times k$.\n",
    "\n",
    "For each of these anchors we'll also want to decide whether we think there's an object present on said region or not (thus, $2 \\times k$ more filters). This will, in essence, look at the activation maps we saw before and decide whether, in a given region, the activated features amount to an object being in there (e.g. many cat ear features have activated).\n",
    "\n",
    "As we saw in the slides, the RPN first has a $3\\times3$ convolutional layer with $512$ filters and then two outputs heads:\n",
    "* One with $2 \\times k$ filters for the **objectness score**.\n",
    "* One with $4 \\times k$ filters for the **encoded deltas**.\n",
    "\n",
    "Both will be implemented as $1 \\times 1$ convolutions in order to support variable-size images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that when implementing Faster R-CNN for training, we should\n",
    "# also specify initializers and regularizers for the weights. We're\n",
    "# omitting them here for brevity.\n",
    "\n",
    "def run_rpn(feature_map):\n",
    "    \"\"\"Run the RPN layers through the feature map.\n",
    "    \n",
    "    Will run the input through an initial convolutional layer of\n",
    "    filter size 3x3 and 512 channels, using the ReLU6 activation.\n",
    "    The output of this layer has the same spatial size as the\n",
    "    input.\n",
    "    \n",
    "    Then run two 1x1 convolutions over this intermediate layer, one\n",
    "    for the resizings and one for the objectness probabilities.\n",
    "    Remember to apply the softmax function over the objectness\n",
    "    scores to get a distribution.\n",
    "    \n",
    "    Arguments:\n",
    "        feature_map: Tensor of shape (1, W, H, C), with WxH the\n",
    "            spatial shape of the feature map and C the number of\n",
    "            channels (1024 in this case).\n",
    "            \n",
    "    Returns:\n",
    "        Tuple of Tensors, with the first being the output of the bbox\n",
    "        resizings `(W * H * num_anchors, 4)` while the second being\n",
    "        the objectness score, of size `(W * H * num_anchors, 2)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: Read the docstring thoroughly to help you pass the correct\n",
    "    # parameters to the conv layers, especially padding (you want to\n",
    "    # keep the *same* spatial size after the initial conv layer).\n",
    "    \n",
    "    # Tip: See the functions `tf.layers.conv2d` and `tf.reshape`. Also\n",
    "    # see `tf.nn.softmax` for the softmax function.\n",
    "        \n",
    "    # The names of the layers should be: `rpn/conv` for the base layer,\n",
    "    # `rpn/cls_conv` for the objectness score, and `rpn/bbox_conv` for\n",
    "    # the bbox resizing.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return rpn_bbox_pred, rpn_cls_prob\n",
    "\n",
    "\n",
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    rpn_bbox_pred, rpn_cls_prob = run_rpn(feature_map)\n",
    "    \n",
    "\n",
    "expected_preds = (\n",
    "    feature_map.shape[1]\n",
    "    * feature_map.shape[2]\n",
    "    * len(ANCHOR_RATIOS)\n",
    "    * len(ANCHOR_SCALES)\n",
    ")\n",
    "\n",
    "assert rpn_bbox_pred.shape[0] == expected_preds, 'Number of proposals should match'\n",
    "assert rpn_cls_prob.shape[0] == expected_preds, 'Number of proposals should match'\n",
    "\n",
    "assert rpn_bbox_pred.shape[1] == 4, 'There should be one delta per bbox coordinate (i.e., four)'\n",
    "assert rpn_cls_prob.shape[1] == 2, 'The objectness score should have two outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a histogram of the bounding box modifications (the deltas) for our current image.\n",
    "\n",
    "Look at the results. Do they make sense? Does it seem that the encoding is indeed helping unbias the predictions? What do values near zero mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = rpn_bbox_pred.numpy()\n",
    "\n",
    "_, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    title = ['D_x', 'D_y', 'D_w', 'D_h'][idx]\n",
    "    ax.set_title(title)\n",
    "    ax.hist(preds[:, idx], bins=50)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the objectness scores. As you'll see, most of the anchors are deemed not worthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = rpn_cls_prob.numpy()[:, 1]\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 6))\n",
    "ax.set_title('Scores (0 = no object, 1 = object)')\n",
    "ax.hist(preds, bins=100)\n",
    "\n",
    "print('{} predictions over 0.9, out of a total of {}'.format(\n",
    "    len(np.flatnonzero(preds > 0.9)), len(preds)\n",
    "))\n",
    "print()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some time left, it may prove insightful to analyze other statistics, such as the objectness and/or resizing by anchor size, or by position in the image. Performing an analysis like this can help pick hyperparameters, guide improvements for the algorithms and find pathologies on the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and filtering proposals\n",
    "\n",
    "We now have the RPN layers outputs as-is. These will be the basis for *regions of interest* that will go through to the next stage of the object detection pipeline. We need, however, do a couple of things with them in order to get there:\n",
    "\n",
    "* Remember that the RPN layers outputs are the **encoded deltas**. So we need to get them back to image pixel space.\n",
    "* Some of the proposals may end up being invalid, as no constraints have been placed on the resizings (aside from the regularization induced by the encoding). For instance, we may end up with **zero-area proposals**, or with the extremes flipped. This may be especially true when we're training the algorithm from scratch, but we're going to filter them just in case.\n",
    "* Finally, many of the proposals may end up being **very** similar to each other. Due to this, we're going to apply an operation called **non-maximum suppression** to filter out proposals that are very similar to each other.\n",
    "\n",
    "Once this is done, we'll be free to continue to the next stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we decode the outputs of the RPN using our previously-implemented `decode` function, obtaining **proposals**. We also get a single-dimension objectness score for each of these proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate proposals from the RPN's output by decoding the bounding boxes\n",
    "# according to the configured anchors.\n",
    "proposals = decode(anchors, rpn_bbox_pred)\n",
    "\n",
    "# Get the (positive-object) scores from the RPN.\n",
    "scores = tf.reshape(rpn_cls_prob[:, 1], [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check and filter proposals that may have zero or negative area (negative area means, in this case, that the bounding box extremes were flipped). It is very much possible that, since we're using fully-trained weights, no proposals with negative area are present. You might want to see the `encode` and `decode` functions you implemented above to see exactly when it can go negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "props = proposals.numpy()\n",
    "areas = (props[:, 2] - props[:, 0]) * (props[:, 3] - props[:, 1])\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(16, 6))\n",
    "ax.set_title('Area per proposal')\n",
    "ax.hist(areas, bins=100)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Proposals with areas under zero:')\n",
    "print(np.flatnonzero(areas <= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N.B.: You might as well skip this step if you're running out of time and\n",
    "# there are no proposals with area under zero, but beware that on a real\n",
    "# implementation ignoring this will cause trouble.\n",
    "\n",
    "def filter_proposals(proposals, scores):\n",
    "    \"\"\"Filters non-positive area proposals.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals: Tensor of shape (num_proposals, 4), holding the\n",
    "            coordinates of the proposals' bounding boxes.\n",
    "        scores: Tensor of shape (num_proposals,), holding the\n",
    "            scores associated to each bounding box.\n",
    "        \n",
    "    Returns:\n",
    "        (`proposals`, `scores`), but with non-positive area proposals removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # See `tf.greater`, `tf.maximum`, `tf.boolean_mask`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return proposals, scores\n",
    "\n",
    "\n",
    "# Filter proposals with negative areas.\n",
    "proposals, scores = filter_proposals(proposals, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use non-maximum suppression on the list of proposals we have. The end result will be a reduced list of proposals (in fact, of size `POST_NMS_TOP_N` defined below), ordered by objectness score, with some redundancy removed (that is, proposals that are too similar to each other will be discarded).\n",
    "\n",
    "As explained in [1], NMS greedily selects a subset of bounding boxes in descending order of score, pruning away boxes that have high intersection-over-union (IOU) [2] overlap with previously selected boxes.\n",
    "\n",
    "We'll be using `NMS_THRESHOLD` as the **IOU overlap threshold**. Also, in order to speed up the NMS (as we may have tens of thousands of proposals, depending on the image size), we'll first limit our proposal list to the top `PRE_NMS_TOP_N` proposals ordered by score.\n",
    "\n",
    "We'll use an already-implemented Tensorflow function for NMS itself. While this avoids the need to code the algorithm, we need to prepare the parameters correctly to feed it.\n",
    "\n",
    "You can read more about non-maximum suppression [here](https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/) and [here](https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/).\n",
    "\n",
    "* [1] https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression\n",
    "* [2] https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit of the initial proposal list, to reduce the number of proposals fed to\n",
    "# non-maximum suppression.\n",
    "PRE_NMS_TOP_N = 12000\n",
    "\n",
    "\n",
    "def keep_top_n(proposals, scores, topn):\n",
    "    \"\"\"Keeps only the top `topn` proposals, as ordered by score.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals: Tensor of shape (num_proposals, 4), holding the\n",
    "            coordinates of the proposals' bounding boxes.\n",
    "        scores: Tensor of shape (num_proposals,), holding the\n",
    "            scores associated to each bounding box.\n",
    "        topn (int): Number of proposals to keep.\n",
    "        \n",
    "    Returns:\n",
    "        (`min(num_proposals, topn)`, `scores`) ordered by score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tip: See `tf.minimum`, `tf.nn.top_k` to get the top values, and\n",
    "    # `tf.gather` to select indices out of a Tensor.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "    \n",
    "    return sorted_top_proposals, sorted_top_scores\n",
    "\n",
    "\n",
    "proposals, scores = keep_top_n(proposals, scores, PRE_NMS_TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the proposals pre-filtered, let's now apply NMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final maximum number of proposals, as returned by NMS.\n",
    "POST_NMS_TOP_N = 2000\n",
    "\n",
    "# IOU overlap threshold for the NMS procedure.\n",
    "NMS_THRESHOLD = 0.7\n",
    "\n",
    "\n",
    "# You might find the following function useful for re-ordening the coordinates\n",
    "# as expected by Tensorflow.\n",
    "def change_order(bboxes):\n",
    "    \"\"\"Change bounding box encoding order.\n",
    "\n",
    "    Tensorflow works with the (y_min, x_min, y_max, x_max) order while we work\n",
    "    with the (x_min, y_min, x_max, y_min).\n",
    "\n",
    "    While both encoding options have its advantages and disadvantages we\n",
    "    decided to use the (x_min, y_min, x_max, y_min), forcing us to switch to\n",
    "    Tensorflow's every time we want to use function that handles bounding\n",
    "    boxes.\n",
    "\n",
    "    Arguments:\n",
    "        bboxes: A Tensor of shape (total_bboxes, 4).\n",
    "\n",
    "    Returns:\n",
    "        bboxes: A Tensor of shape (total_bboxes, 4) with the order swaped.\n",
    "    \"\"\"\n",
    "        \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def apply_nms(proposals, scores):\n",
    "    \"\"\"Applies non-maximum suppression to proposals.\n",
    "    \n",
    "    Arguments:\n",
    "        proposals: Tensor of shape (num_proposals, 4), holding the\n",
    "            coordinates of the proposals' bounding boxes.\n",
    "        scores: Tensor of shape (num_proposals,), holding the\n",
    "            scores associated to each bounding box.\n",
    "        \n",
    "    Returns:\n",
    "        (`proposals`, `scores`), but with NMS applied, and ordered by score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: See `tf.image.non_max_suppression` to perform NMS, our `change_order`\n",
    "    # to prepare the bounding boxes, and `tf.gather` to pick indices out of a\n",
    "    # Tensor.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return proposals, scores\n",
    "\n",
    "pre_merge_proposals, pre_merge_scores = proposals, scores\n",
    "proposals, scores = apply_nms(proposals, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the center positions have changed pre- and post- merging of proposals when restricted to the first $2000$ proposals. After applying NMS, we should have improved our coverage of the image somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k = tf.nn.top_k(pre_merge_scores, k=proposals.shape[0])\n",
    "props = tf.gather(pre_merge_proposals, top_k.indices).numpy()\n",
    "\n",
    "pre_merge_centers = np.stack([\n",
    "    (props[:, 0] + props[:, 2]) / 2,\n",
    "    (props[:, 1] + props[:, 3]) / 2,\n",
    "], axis=1)\n",
    "\n",
    "post_merge_centers = np.stack([\n",
    "    (proposals[:, 0] + proposals[:, 2]) / 2,\n",
    "    (proposals[:, 1] + proposals[:, 3]) / 2,\n",
    "], axis=1)\n",
    "\n",
    "_, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "axes[0][0].set_title('x-axis center positions pre-merge')\n",
    "axes[0][0].hist(pre_merge_centers[:, 0], bins=40)\n",
    "axes[0][1].set_title('y-axis center positions pre-merge')\n",
    "axes[0][1].hist(pre_merge_centers[:, 1], bins=40)\n",
    "axes[1][0].set_title('x-axis center positions post-merge')\n",
    "axes[1][0].hist(post_merge_centers[:, 0], bins=40)\n",
    "axes[1][1].set_title('y-axis center positions post-merge')\n",
    "axes[1][1].hist(post_merge_centers[:, 1], bins=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing up the RPN: what have we detected?\n",
    "\n",
    "This concludes the work on the Region Proposal Network! We now have a mechanism to, given an image of arbitrary size, return **regions of interest** (i.e. proposals), where it looks like an object is present.\n",
    "\n",
    "Having gone through all the steps, from generating anchors around the image to predicting and filtering proposals, we now have a list of `POST_NMS_TOP_N` proposals (two thousand, in this case), each with an objectness score assigned.\n",
    "\n",
    "Two thousand proposals are, of course, many more than what we need. Also, we need to assign an actual class to each of these proposals, or discard them if they're not correct. That will be attacked by the rest of our object detection pipeline.\n",
    "\n",
    "For now, let's take a look at our current results, so we can understand what we have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the first `topn` proposals, as ordered by score.\n",
    "@interact(\n",
    "    nms=Checkbox(value=True, description='Apply NMS'),\n",
    "    topn=pager(200, 1, min=1, value=10, description='Number of proposals')\n",
    ")\n",
    "def draw(nms, topn):\n",
    "    if nms:\n",
    "        p = proposals\n",
    "        s = scores\n",
    "    else:\n",
    "        p = pre_merge_proposals\n",
    "        s = pre_merge_scores\n",
    "        \n",
    "    print('Minimum score: {:.2f}'.format(s[topn]))\n",
    "    return draw_bboxes(image, p[:topn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Standarizing proposals: Region of Interest Pooling\n",
    "\n",
    "As we just said, we have obtained regions of interest for an arbitrarily-sized input image. Thousands of them. And all of them of a different size. As you've probably seen in the last visualization, some of them may be very small while others very big.\n",
    "\n",
    "The objective of this stage is twofold:\n",
    "* Get the proposals, defined in pixel-space coordinates, back to the feature maps.\n",
    "* Get them all into a fixed size so they can later be fed into a fully-connected neural network.\n",
    "\n",
    "This final size of each region of interest will be $7\\times7\\times1024$. $1024$ being the number of filters that our feature map has, while $7\\times7$ corresponds to the common spatial size all proposals will have. (This implies, as you might notice, that the aspect ratio of the proposals will change.)\n",
    "\n",
    "On the original Faster R-CNN paper, a technique called RoI pooling is used. Here, instead, we use the `tf.image.crop_and_resize` Tensorflow function, which is (in performance terms) almost equivalent but simpler to implement.\n",
    "\n",
    "Also, bear in mind that the RoI pooling layer **first** resizes to *double* of the pooling size (i.e. gets regions of $14\\times14$) and then uses max pooling to get the final $7\\times7$ regions. This makes the resulting regions more smooth and makes them capture more details. One could even go further and resize to $28\\times28$ or more, but since we're **making a copy** of the feature map, memory usage will rapidly go up.\n",
    "\n",
    "So much for an introduction. The implementation should be relatively straightforward. So go on ahead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_bboxes(proposals, im_shape):\n",
    "    \"\"\"\n",
    "    Gets normalized coordinates for RoIs (between 0 and 1 for cropping)\n",
    "    in TensorFlow's order (y1, x1, y2, x2).\n",
    "\n",
    "    Arguments:\n",
    "        roi_proposals: A Tensor with the bounding boxes of shape\n",
    "            (total_proposals, 4), where the values for each proposal are\n",
    "            (x_min, y_min, x_max, y_max).\n",
    "        im_shape: A Tensor with the shape of the image (height, width).\n",
    "\n",
    "    Returns:\n",
    "        bboxes: A Tensor with normalized bounding boxes in TensorFlow's\n",
    "            format order. Its should is (total_proposals, 4).\n",
    "    \"\"\"\n",
    "    \n",
    "    # See `tf.unstack`, `tf.stack`, `tf.cast`.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    \n",
    "    ####\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "\n",
    "def roi_pooling(feature_map, proposals, im_shape, pool_size=7):\n",
    "    \"\"\"Perform RoI pooling.\n",
    "\n",
    "    This is a simplified method than what's done in the paper that obtains\n",
    "    similar results. We crop the proposal over the feature map and resize it\n",
    "    bilinearly.\n",
    "    \n",
    "    This function first resizes to *double* of `pool_size` (i.e. gets\n",
    "    regions of (pool_size * 2, pool_size * 2)) and then uses max pooling to\n",
    "    get the final `(pool_size, pool_size)` regions.\n",
    "    \n",
    "    Arguments:\n",
    "        feature_map: Tensor of shape (1, W, H, C), with WxH the spatial\n",
    "            shape of the feature map and C the number of channels (1024\n",
    "            in this case).\n",
    "        proposals: Tensor of shape (total_proposals, 4), holding the proposals\n",
    "            to perform RoI pooling on.\n",
    "        im_shape: A Tensor with the shape of the image (height, width).\n",
    "        pool_size (int): Final width/height of the pooled region.\n",
    "    \n",
    "    Returns:\n",
    "        Pooled feature map, with shape `(num_proposals, pool_size, pool_size,\n",
    "        feature_map_channels)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tip: See `tf.image.crop_and_resize` to get crops out of the feature map\n",
    "    # and resize them. You can ignore the `box_ind` argument by passing an\n",
    "    # array of the correct size filled with zeros (one per proposal).\n",
    "    \n",
    "    # Tip: Remember to perform the max pooling as described above, by using\n",
    "    # the `tf.nn.max_pool` function.\n",
    "    \n",
    "    # N.B.: You can resize to `(pool_size, pool_size)` directly and avoid the\n",
    "    # max pooling step, though the results *will* be inferior.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "\n",
    "    ####\n",
    "\n",
    "    return pooled\n",
    "\n",
    "\n",
    "pooled = roi_pooling(feature_map, proposals, (image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to gain an intuition on what exactly is being done here, let's now visualize our **pooled regions of interest**, along with the image patches they come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = pooled.numpy()\n",
    "\n",
    "# Pool the images too to visualize, but using a higher pooling size so\n",
    "# we don't lose too much resolution.\n",
    "image_crops = roi_pooling(\n",
    "    image, proposals,\n",
    "    (image.shape[1], image.shape[2]),\n",
    "    pool_size=140\n",
    ").numpy().astype(np.uint8)\n",
    "\n",
    "\n",
    "@interact(\n",
    "    fm_idx=pager(pool.shape[-1], 1, 'Feature map index'),\n",
    "    im_idx=pager(pool.shape[0], 25, 'Proposals')\n",
    ")\n",
    "def display_pooled_proposal(fm_idx=0, im_idx=0):\n",
    "    axes = image_grid(25, 5, sizes=(3, 3))\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if im_idx * 25 + idx >= pool.shape[0]:\n",
    "            break\n",
    "            \n",
    "        fm = (\n",
    "            pool[idx, :, :, fm_idx]\n",
    "            / pool[idx, :, :, fm_idx].max()\n",
    "            * 255\n",
    "        ).astype(np.uint8)\n",
    "        \n",
    "        # Get the pooled image regions.\n",
    "        img = image_crops[im_idx * 25 + idx, ...]\n",
    "        \n",
    "        fm_image = Image.fromarray(fm, mode='L').convert('RGBA')\n",
    "        fm_image = fm_image.resize(img.shape[0:2][::-1], resample=Image.NEAREST)\n",
    "\n",
    "        # Add some alpha to overlay it over the image.\n",
    "        fm_image.putalpha(120)\n",
    "\n",
    "        base_image = Image.fromarray(img)\n",
    "        base_image.paste(fm_image, (0, 0), fm_image)\n",
    "        \n",
    "        ax.imshow(base_image, aspect='auto')\n",
    "\n",
    "    plt.subplots_adjust(wspace=.02, hspace=.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using the proposals: Region-CNN\n",
    "\n",
    "We're ready for the final stage! Here we'll be doing two things:\n",
    "* Running our set of fixed-sized proposals through a network akin to what was done in the RPN: one input, two outputs. In this case, instead of an objectness score, the output will be a class score (plus a possible **background** score).\n",
    "* Get these thousands of proposals into a reasonable number. We'll be performing NMS again, but this time per class.\n",
    "\n",
    "As you can see, it looks like more of the same, which (save some details) it effectively is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're finally ready to perform the classification, so load the class names.\n",
    "with open('checkpoint/classes.json') as f:\n",
    "    classes = json.load(f)\n",
    "    \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classification network\n",
    "\n",
    "As we mentioned before, this last stage will get the proposals through a fully-connected layer. However, before doing that, we'll perform a bit more feature extraction.\n",
    "\n",
    "You might remember when you were implementing the RPN that we used the first three out of four blocks of the ResNet for feature extraction, discarding the final block. The reasoning behind this move was to leverage the fact that the block three should, in principle, detect more *abstract features* than the final, block four. Now we're ready to perform the final classification, so we will first pass our proposals (which are cuts, although resized, of the original feature map) through **the block four of the ResNet**.\n",
    "\n",
    "Also, once we do this, since we've already extracted all the features we care about, we'll perform **Global Average Pooling** which means, essentialy, to average out the spatial information: we only care that in a given proposal, some feature was present all around. That means we'll be left with a single $1024$-sized vector per proposals.\n",
    "\n",
    "And, finally, pass this fixed-length vector through two fully-connected layers: one for the bounding box resizings and one for the classes. Since we've used the block four of the ResNet already, we'll not be using an intermediate layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(run_resnet_tail.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_rcnn(pooled, num_classes):\n",
    "    \"\"\"Run the RCNN layers through the pooled features.\n",
    "\n",
    "    This directly applies a fully-connected layer from `features`\n",
    "    to the two outputs we want: a class probability (plus the\n",
    "    background class) and the bounding box resizings (one per\n",
    "    class).\n",
    "    \n",
    "    In order to obtain the class probability, we apply a softmax\n",
    "    over the scores obtained from the dense layer, similar to the RPN.\n",
    "    \n",
    "    Arguments:\n",
    "        pooled: Pooled feature map, with shape `(num_proposals,\n",
    "            pool_size, pool_size, feature_map_channels)`.\n",
    "        num_classes: Number of classes for the R-CNN.\n",
    "            \n",
    "    Returns:\n",
    "        Tuple of Tensors, with the first being the output of the\n",
    "        bbox resizings `(W * H * proposals, 4)` and the second being\n",
    "        the class scores, of size `(pool_size ^ 2 * proposals,\n",
    "        num_classes)`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remember, you need to do three things with `pooled`:\n",
    "    # * Pass them through the ResNet block four.\n",
    "    #   (Tip: See the function `run_resnet_tail`s docstring above.)\n",
    "    # * Perform Global Average Pooling.\n",
    "    #   (Tip: See the function `tf.reduce_mean`.)\n",
    "    # * Run them through two fully-connected layers.\n",
    "    #   (Tip: See the functions `tf.layers.dense`, `tf.nn.softmax`.)\n",
    "    \n",
    "    # W.r.t the fully-connected layers, remember:\n",
    "    # * To add an extra class for the background class.\n",
    "    # * To have bounding-box resizings **per-class**.\n",
    "    \n",
    "    # The names of the layers should be: `rcnn/fc_classifier` for\n",
    "    # the classification head, and `rcnn/fc_bbox` for the bbox\n",
    "    # resizing head.\n",
    "    \n",
    "    ####\n",
    "    # Fill this function below, paying attention to the docstring.\n",
    "    ####\n",
    "    features = run_resnet_tail(pooled)\n",
    "    \n",
    "    features = tf.reduce_mean(features, [1, 2])\n",
    "    \n",
    "    rcnn_cls_score = tf.layers.dense(\n",
    "        features,\n",
    "        num_classes + 1,\n",
    "        name='rcnn/fc_classifier',\n",
    "    )\n",
    "\n",
    "    rcnn_cls_prob = tf.nn.softmax(rcnn_cls_score)\n",
    "\n",
    "    rcnn_bbox = tf.layers.dense(\n",
    "        features,\n",
    "        num_classes * 4,\n",
    "        name='rcnn/fc_bbox',\n",
    "    )\n",
    "    ####\n",
    "\n",
    "    return rcnn_bbox, rcnn_cls_prob\n",
    "\n",
    "\n",
    "with tfe.restore_variables_on_create('checkpoint/fasterrcnn'):\n",
    "    bbox_pred, cls_prob = run_rcnn(pooled, len(classes))\n",
    "    \n",
    "    \n",
    "assert bbox_pred.shape[0] == pooled.shape[0], 'Number of proposals should match'\n",
    "assert cls_prob.shape[0] == pooled.shape[0], 'Number of proposals should match'\n",
    "\n",
    "assert bbox_pred.shape[1] == len(classes) * 4, 'There should be 4 bbox resizings per class'\n",
    "assert cls_prob.shape[1] == len(classes) + 1, 'There should be 81 class probabilities (remember the background!)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results now, and see whether they make sense.\n",
    "\n",
    "We'll display the **most probable class for each proposal**, before applying the final class-specific resizing: this is the pooled region of interest, what the classifier actually looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_classes = ['background'] + classes\n",
    "    \n",
    "preds = np.argmax(cls_prob.numpy(), axis=1)\n",
    "\n",
    "@interact(page=pager(len(preds), 20, 'Proposals'))\n",
    "def display_predictions(page):\n",
    "    axes = image_grid(20, 5, sizes=(3, 3))\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if 20 * page + idx >= image_crops.shape[0]:\n",
    "            break\n",
    "            \n",
    "        ax.imshow(image_crops[20 * page + idx, ...], aspect='auto')\n",
    "        ax.set_title(output_classes[preds[20 * page + idx]])\n",
    "\n",
    "    plt.subplots_adjust(wspace=.02, hspace=.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now display the proposals again, but with the **bounding box resizings applied**.\n",
    "\n",
    "Corrections are done per-class. In order to understand how much these predictions vary, we take some proposals and apply the different possible resizings.\n",
    "\n",
    "For each region, we first display the resizing for the most probable class and then for three other random classes. If the most probable class is background, we ignore it.\n",
    "\n",
    "Do you notice anything in particular? Which resizing is the one that fits better to the detected object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target normalization variances to adjust the output of the R-CNN so it trains better.\n",
    "TARGET_VARIANCES = np.array([0.1, 0.1, 0.2, 0.2], dtype=np.float32)\n",
    "\n",
    "# We only consider proposals for which the most-probable class was non-background.\n",
    "preds = np.argmax(cls_prob.numpy(), axis=1)\n",
    "non_background = (preds != 0)\n",
    "\n",
    "non_bg_preds = preds[non_background]\n",
    "non_bg_proposals = proposals.numpy()[non_background]\n",
    "non_bg_bboxes = bbox_pred.numpy()[non_background]\n",
    "non_bg_count = len(np.flatnonzero(non_background))\n",
    "\n",
    "\n",
    "@interact(page=pager(non_bg_count, 3, 'Proposals'))\n",
    "def display_resizings(page):\n",
    "    _, axes = plt.subplots(3, 5, figsize=(16, 10))\n",
    "    \n",
    "    for row_idx, cols in enumerate(axes):\n",
    "        for col in cols:\n",
    "            col.axis('off')\n",
    "\n",
    "        proposal_idx = 3 * page + row_idx    \n",
    "        if proposal_idx >= non_bg_count:\n",
    "            continue\n",
    "        \n",
    "        # Original region.\n",
    "        # (Using original region size so comparison is easier to the eye.)\n",
    "        x_min, y_min, x_max, y_max = clip_boxes(\n",
    "            non_bg_proposals[proposal_idx:proposal_idx + 1],\n",
    "            image.shape[1:3]\n",
    "        )[0].numpy().astype(np.int)\n",
    "        cols[0].imshow(image[0, y_min:y_max, x_min:x_max, :])\n",
    "        cols[0].set_title('Region')\n",
    "        \n",
    "        # Per-class region, correct class first.\n",
    "        class_ids = np.concatenate([\n",
    "            np.array([non_bg_preds[proposal_idx] - 1]),\n",
    "            np.random.randint(0, len(classes), 3)\n",
    "        ])\n",
    "        for col, class_id in zip(cols[1:], class_ids):\n",
    "            cls_bbox_pred = non_bg_bboxes[\n",
    "                proposal_idx:proposal_idx + 1,\n",
    "                (4 * class_id):(4 * class_id + 4)\n",
    "            ]\n",
    "\n",
    "            cls_objects = decode(\n",
    "                non_bg_proposals[proposal_idx:proposal_idx+1],\n",
    "                cls_bbox_pred * TARGET_VARIANCES\n",
    "            ).numpy()\n",
    "            \n",
    "            x_min, y_min, x_max, y_max = clip_boxes(\n",
    "                cls_objects, image.shape[1:3]\n",
    "            )[0].numpy().astype(np.int)\n",
    "\n",
    "            col.imshow(image[0, y_min:y_max, x_min:x_max, :])\n",
    "            col.set_title(classes[class_id])\n",
    "        \n",
    "    plt.subplots_adjust(wspace=.02, hspace=.15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the object proposals\n",
    "\n",
    "We're finally getting there! We have one last step to do: getting the final predictions.\n",
    "\n",
    "What we have now is a list of `POST_NMS_TOP_N` proposals (around $2000$), each with 81 class scores (80 classes plus the background) and 80 bounding box resizings. Out of this, we'll consider a total of $2000 \\times 80 = 160000$ proposals, each with a **score** (its class score) and **bounding box resizing**. We do this in order to consider **all** possible classifications for a given bounding box: if the most-probable class of a bounding box has a score of $0.48$ and the second one has a score of $0.47$, it is important to consider **both** variants, and not just the highest-scored one.\n",
    "\n",
    "Of course, we'll not really build the $160000$ proposals at once, but instead perform NMS **on a class-by-class basis**, keeping only the top $100$ proposals per class. Then we'll order all the proposals and keep only the top $300$, which will be the output of our algorithm.\n",
    "\n",
    "**We've already implemented all this part for you**, as it's same as above but on a class-by-class basis. With a little work, you should be able to do it by yourself if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objects, labels, probs = rcnn_proposals(\n",
    "    proposals, bbox_pred, cls_prob, image.shape[1:3], 80,\n",
    "    min_prob_threshold=0.0,\n",
    ")\n",
    "\n",
    "objects = objects.numpy()\n",
    "labels = labels.numpy()\n",
    "probs = probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of detections above 0.1 probability:', len(labels[probs > 0.1]))\n",
    "print('Number of detections above 0.5 probability:', len(labels[probs > 0.5]))\n",
    "print('Number of detections above 0.7 probability:', len(labels[probs > 0.7]))\n",
    "print()\n",
    "\n",
    "# Accumluated probability score per class.\n",
    "probs_per_class = np.bincount(labels, weights=probs, minlength=len(classes))\n",
    "top_n = probs_per_class.argsort()[::-1][:5]\n",
    "print('Top 5 predicted classes:')\n",
    "for cls_idx in top_n:\n",
    "    print('   {}: {} ({:.2f})'.format(cls_idx, classes[cls_idx], probs_per_class[cls_idx]))\n",
    "    \n",
    "_, ax = plt.subplots(1, figsize=(16, 4))\n",
    "ax.bar(np.arange(80), probs_per_class)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, however, we don't want $300$ low-quality detections, but whatever is good. What we do in practice is to filter detections by their probability score (which is the proposal's class score).\n",
    "\n",
    "Let's take a look at the $300$ proposals and see how much the predictions change when filtering by score. In the real world, you'll probably use a threshold above $0.5$, depending on your desired precision vs. recall trade-off (too high a threshold will miss detections, while too low will add noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slider = FloatSlider(\n",
    "    min=0.0, max=1.0, step=0.01, value=0.7,\n",
    "    description='Probability threshold',\n",
    "    layout=Layout(width='600px'),\n",
    "    style={'description_width': 'initial'},\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "@interact(prob=slider)\n",
    "def display_objects(prob):\n",
    "    MAX_TO_DRAW = 50\n",
    "\n",
    "    mask = probs > prob\n",
    "\n",
    "    return draw_bboxes_with_labels(\n",
    "        image, classes,\n",
    "        objects[mask][:MAX_TO_DRAW],\n",
    "        labels[mask][:MAX_TO_DRAW],\n",
    "        probs[mask][:MAX_TO_DRAW],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summing up\n",
    "\n",
    "**Congratulations!** You finished your own implementation of Faster R-CNN, one of the state-of-the-art object detection algorithms.\n",
    "\n",
    "Throughout this notebook you should have learned quite a few things:\n",
    "* **How modern object detectors work**: what inputs they take, what kinds of operations and logic they do, and how much control we have in their workings.\n",
    "* In particular, **how Faster R-CNN works**, very much in depth.\n",
    "* **How to use Tensorflow and numpy in the context of computer vision and object detection**. Going a bit more than the usual \"stack three layers and call it a day\": we've worked with arbitrarily-sized inputs, used conditionals, filtering and other non-standard functions, all within the Tensorflow graph (meaning it can run entirely within a GPU).\n",
    "* **How to visualize the inner workings of an object detection pipeline**. By leveraging an already-trained network, we could see and corroborate each step of the pipeline to understand what goes behind the scenes and whether we made any errors. This process may have also given you some clues in how to improve the algorithm itself.\n",
    "\n",
    "This, of course, is just the beginning. Some things you could try, going forward, are:\n",
    "* **Implement the training of a Faster R-CNN model**. We barely touched on this part, using a pre-trained checkpoint provided by us. The training, apart from using the autograd of your favorite deep learning library, requires some extra steps:\n",
    "\n",
    "    * Implement the **targets**. We're using supervised learning to train this, so how does training data fit into this? We need to train both the RPN and the R-CNN. In order to do this, we need to build mini-batches of training data for both components, by matching ground-truth boxes to our proposals. You can learn how this is done in our implementation of [Luminoth](https://github.com/tryolabs/luminoth).\n",
    "\n",
    "    * Implement the **loss functions**. Once the targets are in place, we need to select good losses and balance our dataset in order to train correctly. There may be some difficulties when training, as we are optimizing four losses in total (two for the RPN, two for the R-CNN).\n",
    "\n",
    "* Improve the algorithm itself. Faster-RCNN has been out for a while now, and while it's still very competitive, there are some known improvements to do. For instance, the RPN can be replaced entirely with the **Feature Pyramid Network** (FPN) [1] and the loss exchanged with the **Focal Loss** [2], to obtain the algorithm called **RetinaNet**.\n",
    "\n",
    "\n",
    "* [1] https://arxiv.org/pdf/1612.03144.pdf\n",
    "* [2] https://arxiv.org/pdf/1708.02002.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
